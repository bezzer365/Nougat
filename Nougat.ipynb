{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "16jGD6ECSurpVAZgKoItGzuJRDK0iIz3S",
      "authorship_tag": "ABX9TyOR5iHEuMb5pTBq6d+JMqBD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bezzer365/Nougat/blob/main/Nougat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-ZLR99r_LGFo",
        "outputId": "be861e0f-d6eb-468b-a74f-50d3feaad202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nougat-ocr\n",
            "  Downloading nougat_ocr-0.1.17-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr) (4.50.0)\n",
            "Collecting timm==0.5.4 (from nougat-ocr)\n",
            "  Downloading timm-0.5.4-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from nougat-ocr) (3.10.15)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from nougat-ocr) (4.11.0.86)\n",
            "Collecting datasets[vision] (from nougat-ocr)\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting lightning<2022,>=2.0.0 (from nougat-ocr)\n",
            "  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from nougat-ocr) (3.9.1)\n",
            "Collecting python-Levenshtein (from nougat-ocr)\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from nougat-ocr) (0.2.0)\n",
            "Collecting sconf>=0.2.3 (from nougat-ocr)\n",
            "  Downloading sconf-0.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: albumentations>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr) (2.0.5)\n",
            "Collecting pypdf>=3.1.0 (from nougat-ocr)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pypdfium2 (from nougat-ocr)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4->nougat-ocr) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4->nougat-ocr) (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.0.0->nougat-ocr) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.0.0->nougat-ocr) (1.14.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.0.0->nougat-ocr) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.0.0->nougat-ocr) (2.10.6)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.0.0->nougat-ocr) (0.0.23)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations>=1.0.0->nougat-ocr) (3.12.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations>=1.0.0->nougat-ocr) (6.2.1)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<2022,>=2.0.0->nougat-ocr) (2025.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2022,>=2.0.0->nougat-ocr)\n",
            "  Downloading lightning_utilities-0.14.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr) (24.2)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<2022,>=2.0.0->nougat-ocr)\n",
            "  Downloading torchmetrics-1.7.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning<2022,>=2.0.0->nougat-ocr)\n",
            "  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting ruamel.yaml (from sconf>=0.2.3->nougat-ocr)\n",
            "  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting munch (from sconf>=0.2.3->nougat-ocr)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr) (0.29.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets[vision]->nougat-ocr)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr) (2.2.2)\n",
            "Collecting xxhash (from datasets[vision]->nougat-ocr)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets[vision]->nougat-ocr)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<2026.0,>=2022.5.0 (from fsspec[http]<2026.0,>=2022.5.0->lightning<2022,>=2.0.0->nougat-ocr)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr) (3.11.14)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr) (11.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr) (1.4.2)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein->nougat-ocr)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein->nougat-ocr)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr) (1.18.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2022,>=2.0.0->nougat-ocr) (75.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations>=1.0.0->nougat-ocr) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations>=1.0.0->nougat-ocr) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->nougat-ocr) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->nougat-ocr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->nougat-ocr) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->nougat-ocr) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4->timm==0.5.4->nougat-ocr)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4->timm==0.5.4->nougat-ocr)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4->timm==0.5.4->nougat-ocr)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4->timm==0.5.4->nougat-ocr)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4->timm==0.5.4->nougat-ocr)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4->timm==0.5.4->nougat-ocr)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.5.4->nougat-ocr) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr) (2025.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->sconf>=0.2.3->nougat-ocr)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[vision]->nougat-ocr) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->timm==0.5.4->nougat-ocr) (3.0.2)\n",
            "Downloading nougat_ocr-0.1.17-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.1-py3-none-any.whl (818 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sconf-0.2.5-py3-none-any.whl (8.8 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.2-py3-none-any.whl (28 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.0-py3-none-any.whl (960 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, ruamel.yaml.clib, rapidfuzz, pypdfium2, pypdf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, lightning-utilities, fsspec, dill, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, Levenshtein, sconf, python-Levenshtein, nvidia-cusolver-cu12, datasets, torchmetrics, timm, pytorch-lightning, lightning, nougat-ocr\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.15\n",
            "    Uninstalling timm-1.0.15:\n",
            "      Successfully uninstalled timm-1.0.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.27.1 datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 lightning-2.5.1 lightning-utilities-0.14.2 multiprocess-0.70.16 munch-4.0.0 nougat-ocr-0.1.17 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pypdf-5.4.0 pypdfium2-4.30.1 python-Levenshtein-0.27.1 pytorch-lightning-2.5.1 rapidfuzz-3.12.2 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 sconf-0.2.5 timm-0.5.4 torchmetrics-1.7.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nougat-ocr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/facebookresearch/nougat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RnGiOskyNAJk",
        "outputId": "a8a08e19-26d8-4599-9617-197682130ca2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/nougat\n",
            "  Cloning https://github.com/facebookresearch/nougat to /tmp/pip-req-build-e60qd2lz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/nougat /tmp/pip-req-build-e60qd2lz\n",
            "  Resolved https://github.com/facebookresearch/nougat to commit 5a92920d342fb6acf05fc9b594ccb4053dbe8e7a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<=4.38.2,>=4.25.1 (from nougat-ocr==0.1.18)\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.5.4 (from nougat-ocr==0.1.18)\n",
            "  Downloading timm-0.5.4-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.10.15)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (4.11.0.86)\n",
            "Collecting datasets[vision] (from nougat-ocr==0.1.18)\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting lightning<2022,>=2.0.0 (from nougat-ocr==0.1.18)\n",
            "  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.9.1)\n",
            "Collecting rapidfuzz (from nougat-ocr==0.1.18)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (0.2.0)\n",
            "Collecting sconf>=0.2.3 (from nougat-ocr==0.1.18)\n",
            "  Downloading sconf-0.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting albumentations<=1.4.24,>=1.0.0 (from nougat-ocr==0.1.18)\n",
            "  Downloading albumentations-1.4.24-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting pypdf>=3.1.0 (from nougat-ocr==0.1.18)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pypdfium2 (from nougat-ocr==0.1.18)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4->nougat-ocr==0.1.18) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4->nougat-ocr==0.1.18) (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations<=1.4.24,>=1.0.0->nougat-ocr==0.1.18) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations<=1.4.24,>=1.0.0->nougat-ocr==0.1.18) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations<=1.4.24,>=1.0.0->nougat-ocr==0.1.18) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations<=1.4.24,>=1.0.0->nougat-ocr==0.1.18) (2.10.6)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations<=1.4.24,>=1.0.0->nougat-ocr==0.1.18) (0.0.23)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations<=1.4.24,>=1.0.0->nougat-ocr==0.1.18) (3.12.2)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations<=1.4.24,>=1.0.0->nougat-ocr==0.1.18) (6.2.1)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (2024.10.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18)\n",
            "  Downloading lightning_utilities-0.14.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (24.2)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18)\n",
            "  Downloading torchmetrics-1.7.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18)\n",
            "  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting ruamel.yaml (from sconf>=0.2.3->nougat-ocr==0.1.18)\n",
            "  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting munch (from sconf>=0.2.3->nougat-ocr==0.1.18)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<=4.38.2,>=4.25.1->nougat-ocr==0.1.18) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.38.2,>=4.25.1->nougat-ocr==0.1.18) (0.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.38.2,>=4.25.1->nougat-ocr==0.1.18) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers<=4.38.2,>=4.25.1->nougat-ocr==0.1.18) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers<=4.38.2,>=4.25.1->nougat-ocr==0.1.18)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.38.2,>=4.25.1->nougat-ocr==0.1.18) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets[vision]->nougat-ocr==0.1.18)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (2.2.2)\n",
            "Collecting xxhash (from datasets[vision]->nougat-ocr==0.1.18)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets[vision]->nougat-ocr==0.1.18)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (3.11.13)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (11.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.18) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.18) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.18.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (75.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations<=1.4.24,>=1.0.0->nougat-ocr==0.1.18) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations<=1.4.24,>=1.0.0->nougat-ocr==0.1.18) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<=4.38.2,>=4.25.1->nougat-ocr==0.1.18) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<=4.38.2,>=4.25.1->nougat-ocr==0.1.18) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<=4.38.2,>=4.25.1->nougat-ocr==0.1.18) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<=4.38.2,>=4.25.1->nougat-ocr==0.1.18) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2025.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->sconf>=0.2.3->nougat-ocr==0.1.18)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[vision]->nougat-ocr==0.1.18) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->timm==0.5.4->nougat-ocr==0.1.18) (3.0.2)\n",
            "Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albumentations-1.4.24-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.1-py3-none-any.whl (818 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sconf-0.2.5-py3-none-any.whl (8.8 kB)\n",
            "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.2-py3-none-any.whl (28 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.0-py3-none-any.whl (960 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nougat-ocr\n",
            "  Building wheel for nougat-ocr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nougat-ocr: filename=nougat_ocr-0.1.18-py3-none-any.whl size=81723 sha256=5e0d2188d774bfa322fda9e58e0b9085f0c22b1335bc8c9209511c2256add54a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9e1yah78/wheels/a8/b1/ca/3509cca72deee3fab5cf02371a58b5d57b82b1c99656402e8c\n",
            "Successfully built nougat-ocr\n",
            "Installing collected packages: xxhash, ruamel.yaml.clib, rapidfuzz, pypdfium2, pypdf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, lightning-utilities, dill, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tokenizers, sconf, nvidia-cusolver-cu12, albumentations, transformers, datasets, torchmetrics, timm, pytorch-lightning, lightning, nougat-ocr\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.5\n",
            "    Uninstalling albumentations-2.0.5:\n",
            "      Successfully uninstalled albumentations-2.0.5\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.15\n",
            "    Uninstalling timm-1.0.15:\n",
            "      Successfully uninstalled timm-1.0.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed albumentations-1.4.24 datasets-3.4.1 dill-0.3.8 lightning-2.5.1 lightning-utilities-0.14.2 multiprocess-0.70.16 munch-4.0.0 nougat-ocr-0.1.18 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pypdf-5.4.0 pypdfium2-4.30.1 pytorch-lightning-2.5.1 rapidfuzz-3.12.2 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 sconf-0.2.5 timm-0.5.4 tokenizers-0.15.2 torchmetrics-1.7.0 transformers-4.38.2 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display"
      ],
      "metadata": {
        "id": "6szGC3NLLMfy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nougat '/content/PDFs/Adv Eng Mater - 2024 - Berry.pdf' --o 'Output'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Z2FnM6MFQl",
        "outputId": "b70be6c9-f77f-4a90-a4a7-7c0a197bcb0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "/usr/local/lib/python3.11/dist-packages/nougat/transforms.py:146: UserWarning: Argument 'alpha_affine' is not valid and will be ignored.\n",
            "  alb.ElasticTransform(\n",
            "downloading nougat checkpoint version 0.1.0-small to path /root/.cache/torch/hub/nougat-0.1.0-small\n",
            "config.json: 100% 557/557 [00:00<00:00, 3.38Mb/s]\n",
            "pytorch_model.bin: 100% 956M/956M [00:09<00:00, 106Mb/s]\n",
            "special_tokens_map.json: 100% 96.0/96.0 [00:00<00:00, 404kb/s]\n",
            "tokenizer.json: 100% 2.04M/2.04M [00:00<00:00, 44.9Mb/s]\n",
            "tokenizer_config.json: 100% 106/106 [00:00<00:00, 690kb/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "  0% 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/nougat/model.py:437: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
            "  return torch.var(self.values, 1) / self.values.shape[1]\n",
            "WARNING:root:Found repetitions in sample 0\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "INFO:root:Processing file /content/PDFs/Adv Eng Mater - 2024 - Berry.pdf with 18 pages\n",
            "WARNING:root:Skipping page 1 due to repetitions.\n",
            " 20% 1/5 [00:22<01:28, 22.22s/it]WARNING:root:Found repetitions in sample 1\n",
            "WARNING:root:Skipping page 6 due to repetitions.\n",
            "100% 5/5 [02:04<00:00, 24.84s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can add \"-- no-skipping\" to the end to prevent the skipping error\n",
        "# Add \"--recompute\" to run the same file again\n",
        "!nougat '/content/PDFs/Adv Eng Mater - 2024 - Berry.pdf' --o 'Output' --no-skipping --recompute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOXbAkkiM10t",
        "outputId": "7294d3e2-94a3-4199-f3c0-843f148025af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "/usr/local/lib/python3.11/dist-packages/nougat/transforms.py:146: UserWarning: Argument 'alpha_affine' is not valid and will be ignored.\n",
            "  alb.ElasticTransform(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "  0% 0/5 [00:00<?, ?it/s]INFO:root:Processing file /content/PDFs/Adv Eng Mater - 2024 - Berry.pdf with 18 pages\n",
            "100% 5/5 [03:01<00:00, 36.28s/it]\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display.Latex('/content/Output/Adv Eng Mater - 2024 - Berry.mmd')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "U3AxAG75OfF-",
        "outputId": "43e326ac-001a-4f49-98bd-d47047fbf9b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "Design and Selection of High Entropy Alloys for Hardmetal Matrix Applications Using a Coupled Machine Learning and Calculation of Phase Diagrams Methodology\n\nJoshua Berry, Robert Snell, Magnus Anderson, Lewis R. Owen, Olivier M. D. M. Messe, Iain Todd, and Katerina A. Christofidou\n\nJ. Berry, R. Snell, L. R. Owen, I. Todd, K. A. Christofidou Department of Materials Science and Engineering University of Sheffield Mappin Street, Sheffield S1 3JD, UK (j. Berry)@sheffield.ac.uk; kchristofidou@sheffield.ac.uk M. Anderson Thermo-Calc Software AB Rasundagarajan 18, S5-169 67 Solna, Sweden (j. Berry) O. M. M. Messe\n\nO. D. M. Messe\n\nOliivier M. D. M. Messe, Iain Todd, and Katerina A. Christofidou\n\n###### Abstract\n\nThis study aims to utilize a combined machine learning (ML) and CALculation of PHAse Diagrams (CALPHAD) methodology to design hardmetal matrix phases for metal-forming applications that can serve as the basis for carbide reinforcement. The vast compositional space that high entropy alloys (HEAs) occupy offers a promising avenue to satisfy the application design criteria of wear resistance and ductility. To efficiently explore this space, random forest ML models are constructed and trained from publicly available experimental HEA databases to make phase constitution and hardness predictions. Interrogation of the ML models constructed reveals accuracies \\(>\\)78.7% and a mean absolute error of 66.1 HV for phase and hardness predictions respectively. Six promising alloy compositions, extracted from the ML predictions and CALPHAD calculations, are experimentally fabricated and tested. The hardness predictions are found to be systematically under- and overpredicted depending on the alloy microstructure. In parallel, the phase classification models are found to lack sensitivity toward additional intermetallic phase formation. Despite the discrepancies identified between ML and experimental results, the fabricated compositions show promise for further experimental evaluation. These discrepancies are believed to be directly associated with the available databases but, importantly, have highlighted several avenues for both ML and database development.\n\nMachine Learning, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys, High Entropy Alloys,\n\n## 1 Advanced Science News\n\n### _www.advancedsciencenews.com_\n\nHEA microstructure studies investigate as-cast alloys, which do not represent the equilibrium state and are not always indicative of industry applications. Annealing tends to result in microstructures consisting of multiple solid solution and intermetallic phases, even in cases where the as-cast microstructure was single phase.[91]\n\nIn addition to these challenges and limitations, the great opportunities offered by the emergence of HEAs are further complicated by the lack of robust computational tools able to rapidly explore the compositional space for areas of interest. This in turn offers an opportunity for the application of machine learning (ML) tools to accelerate and automate the compositional exploration tasks. Application of ML in material science is not a novel concept and has become accepted as a useful tool to help automate material discovery.[20, 21] Accordingly, a natural relationship has emerged between ML and the HEA field. ML provides an opportunity to explore the vast HEA compositional space,[22] reviewing large amounts of data to discover patterns and trends, and make predictions on unexplored HEA compositions.[23] These predictions can be performed quickly, providing reproducible results, and aiding in alloy design and development, with the capability for future scaling.[24]\n\nSeveral studies of the application of ML to HEAs have been published, mostly aiming at the design of new compositions for structural applications through the determination of alloy microstructures and properties.[25, 23, 24, 25, 26] Most studies utilize existing ML algorithms and architectures, including support vector machines, artificial neural networks, gradient boosting, k-nearest neighbors, and random forest (RF).[28, 29, 30] ML can be trained from available HEA data to make direct predictions on the phase formation and mechanical properties of HEA compositions.[20, 21] New data and features can also be directly added to update and refit the model if they become available.[11] Despite the apparent \"black box\" nature of ML algorithms, multiple techniques such as permutation importance and SHapley Additive explanations (SHAP) can be utilized to explain both the global model methods and local individual predictions respectively, as well as assist in interpretability of feature importance.[24] Future experimental testing can then assess the alloys suitability for application and validate the ML predictions.\n\nDespite the distinct advantages discussed, there are drawbacks to the application of ML in the HEA field. First, the success and development of ML within the HEA field is intrinsically linked to the experimental exploration of the compositional space.[20] Hence, the investigation of HEA systems sampling a wide range of the compositional space is needed for the generation and growth of robust experimental HEA databases. Diverse and expansive databases are required to effectively train the supervised ML algorithms, but HEA databases typically only contain anywhere from a few hundred, to a thousand experimental data points.[22] Hence, the amount of training data is typically insufficient for most algorithms and many HEA ML studies report that, the unavailability of HEA data for training is a major limitation of the models.[12] In addition, the data that are available are typically imbalanced, a direct consequence of the focus on a specific family of elements. Imbalanced data can negatively impact ML predictions by biasing models toward the majority class. Solutions such as oversampling to increase the amounts of the minority class or downsampling to reduce the amount of the majority class often result in additional drawbacks, such as reduction in the useful training dataset. The second major difficulty with the application of ML to HEAs, is the need to produce physically meaningful descriptors to best represent the alloys.[22] ML depends upon the mathematical representation of the materials, their features, which serve as the inputs to the algorithm.[29] Good features should describe the alloys such that the chosen algorithm selects the key information to make predictions from it.[31] Additionally, ML calculations are particularly vulnerable to overfitting. Overfitting occurs when an ML algorithm too closely matches the training dataset and may be unable to effectively make predictions on unseen data.[21]\n\nA common theme that emerges from these studies to address the lack of phase data is the use and integration with CALculation of PHAe Diagrams (CALPHAD) based techniques and predictions.[23, 24, 25, 26, 27] CALPHAD offers a tool that allows the calculations of several microstructural indicators from thermodynamic principles as well as, in some limited cases, and with varying levels of success, the calculation of material properties such as yield strength. These data can be combined with experimental results to enhance the volume of data available for improved ML implementation. Alternatively, CALPHAD outputs are often used as input features for the ML algorithms. However, the accuracy of CALPHAD depends strongly on the reliability of the thermodynamic databases[36] and integrating CALPHAD data with experimental data could bias the outcome of ML predictions. CALPHAD also offers a tool primarily for the prediction of equilibrium phases and as such, in cases where manufacturing methods result in nonequilibrium structures, e.g., for coatings, CALPHAD can yield unreliable data. Furthermore, the generation of a database using the CALPHAD method is both time consuming and computationally expensive.\n\nTo explore alternative uses of CALPHAD and ML in alloy selection, this study aims to establish an alloy design methodology that utilizes simple ML architectures trained on experimental databases. CALPHAD is instead integrated as a postprocessing tool to provide further insights into alloy phase formation, enabling alloy downselection.\n\nTo establish this novel methodology, a case study within the HEA field was chosen, centering on the development of hardmetal matrix phases for metal-forming applications that could serve as the basis for further carbide reinforcement to derive improved performance. This case study emphasizes hardness for improved wear resistance and a face-centered cubic (FCC) phase to maximize ductility as body-centered cubic (BCC) phases are typically observed to display a higher hardness,[37] but exhibit lower ductility compared to FCC phases.[7, 23, 38] For the purposes of this case study, single-phase solid solutions were deemed most beneficial to reduce the number of interfaces present. This case study was simplified to predictions of only phase formation and hardness to enable the easy assesment and demonstration of ML performance.\n\n## 2 Aim of Work\n\nThe aim of this work was to develop a robust methodology, presented in **Figure 1**, for the investigation of HEA compositional space, for the design of structural alloys. The methodology was applied to the design of hardmetal matrix compositions as a casestudy for the development of phase and hardness prediction models. An RF ML architecture was trained from available experimental HEA phase formation and mechanical property data to make high-throughput predictions on the phase formation and hardness of potentially suitable unseen HEA compositions. Additionally, the CALPHAD method was used as a postprocessing technique to interrogate the ML predictions of phase formation for the downselection of novel and potentially suitable HEA compositions for experimental assessment. Consequently, six alloy compositions were selected from the high-throughput analysis for experimental evaluation to assess their suitability for the use case selected, validate the ML methodology, and explore further modifications to enable increased complexity to be built into the models. Albeit a derivative first step to allow the development of algorithms and architectures, this methodology presents an opportunity for the design and development of novel alloy compositions for a host of different structural and functional applications.\n\n## 3 Machine Learning\n\nSeveral studies utilizing ML for the design of HEAs have been reported in the literature and the reader is referred to the comprehensive reviews by Liu et al.[39, 40] Arroyave[41] also provided a detailed discussion on ML for phase prediction and stability. The majority of early ML studies focus on the prediction of a single property, either phase formation or hardness. For example, Huang et al.[21] trialed three different ML models, _k_-nearest neighbors, support vector machines, and a neural network to predict phase formation from a database of 401 alloy compositions. Good performance was achieved, with the neural network producing a prediction accuracy in excess of 80% on average. However, the phase prediction in this study is simplified to classification into three classes consisting of solid solution, intermetallic, or solid solution plus intermetallic. Similarly, Kaufmann et al.[31] applied an RF model to predict phase formation from a database of 1798 alloy compositions. A novel model confidence measure was applied to assess model performance, achieving 75% confidence in model predictions. But, only 134 of the data points were experimentally determined, utilizing DFT to supplement the available data with a further 1664 compositions. For prediction of HEA hardness, Yang et al.[44] again trialed several ML algorithms and determined support vector machines to be the best model on their dataset of 370 data points. High model prediction performance with a root mean square error (RMSE) of 75 and coefficient of determination of 0.94 was reported. Beniwal et al.[43] apply an ensemble of 165 artificial neural networks to predict hardness of HEA systems from a dataset of 218 data points constructed by Gorsse et al.[144] Good model performance was reported with a mean absolute error (MAE) of 82.8 HV and the nature of model predictions was probed further, investigating the impact of each feature on predicted hardness over continuous composition variations.\n\nIn addition to the models constructed with a single target output described above, many studies have begun to utilize ML to predict multiple outputs of microstructural and mechanical properties for the design of new structural HEAs. For example, Huang et al.[45] compared five common ML algorithms for hardness and solid-solution single-phase formation, with RF producing the best performance on a dataset of 106 alloy compositions. Good model performance was achieved and ten alloys were experimentally fabricated to validate the model predictions. However, this study limited the experimental search to two HEA systems to simplify the alloy fabrication process. Furthermore, both Jain et al.[46] and Shen et al.[47] utilize ML to predict both phase formation and hardness. Jain et al.[46] utilized two different models, an extra trees classifier and artificial neural network, trained on 1120 and 99 data points to predict phase formation and hardness, respectively. In contrast, Shen et al.[47] employed an XGBoost model to predict both phase formation and hardness from a dataset containing over 500 data points. Good model performance was achieved by both with phase prediction accuracies around 90% and higher, and MAEs less than 35 HV.\n\nFigure 1: A schematic of the alloy design methodology used in this study, utilizing both ML and CALPHAD for compositional downselection.\n\n## 3 Advanced Science News\n\nThe literature outlined above indicates that successful ML models, comprising architectures from support vector machines to artificial neural networks, on a range of data and input features, can be constructed to predict microstructural and mechanical properties of HEAs. The purpose of the study herein was twofold. First, to compare and evaluate two models constructed with the same ML algorithms from two distinct experimental HEA databases for phase prediction. Second, utilizing the same data source and model architecture to compare phase and hardness prediction models. Interpretability metrics were extracted to understand the equivalence of feature importance between the two models.\n\n### Database Selection\n\nTwo independent experimental HEA databases were selected to train and test the RF model. The database produced by Gorsse et al.[44] - Database on mechanical properties of high entropy alloys and complex concentrated alloys,' contains experimental data collected from studies on 370 alloys and was chosen for its unique inclusion of mechanical property information. A corresponding was published for this database with a new dataset that both corrects errors and includes previously omitted data.[45] The updated dataset was used in this study. The second database, produced by Machaka et al.[46] - Machine learning-based prediction of phases in high-entropy alloys: A data article,\" was selected as at the time of this study it was the most recently published HEA dataset and contains data on 1360 alloys with extensive phase formation information, albeit lacking in property data.\n\n### Data Cleaning and Processing\n\nThe database produced by Gorsse et al.[44] contained 123 compositions with missing hardness data. Following the approach by Huang et al.[47] and Wen et al.[32] to maximize the amount of training data available for hardness predictions, an empirical relationship, given by Equation (1), was utilized to extrapolate hardness values from the yield strength provided in the database, where \\(\\sigma_{y}\\) denotes the yield strength and HV denotes the Vickers hardness. This relationship between yield strength and hardness has been shown to be approximately obeyed by BCC HEAs.[47, 46, 48] Despite application of this relationship, 28 compositions still had missing hardness data. The average of the hardness values available in the dataset was imputed for these compositions. The Machaka et al.[46] database contained extensive phase formation data for every composition, but seventeen compositions had to be omitted from the data used to train the ML model. Data were omitted where the compositions were outside the compositional space considered for the case study herein.\n\nThe Gorsse database is important as it contains both mechanical property data and a richer description of the constituent phases, but contains no processing information; while the Machaka database contains sporadic processing information, but no mechanical property data and a more simplified description of the phases present. Given these limitations, in the current study it was deemed appropriate to include all the available data. Ideally, a subset of the data accounting for processing history would be chosen to train the ML. However, the databases available and used in this study are insufficiently complete to enable such a reduction in training data. This highlights a deficiency in the databases currently available. Development and expansion of future databases should be focused on including a more holistic processing history, enabling manufacturing to be taken into account in the implementation of ML.\n\n\\[\\sigma_{y}(\\text{MPa})\\approx\\frac{1}{3}\\,H(\\text{MPa})=\\frac{9.81}{3}\\,\\text{ HV}(\\text{Hv}) \\tag{1}\\]\n\n### Machine Learning Input Features\n\nThe training of the ML models and their use in the prediction of hardness and phase formation of new HEA compositions were based upon the physical properties of the constituent elements of the alloys. Properties such as atomic radii and valence electrons and bulk elemental properties, such as melting temperature and Young's modulus, were used. These physical properties were transformed into features that mathematically describe the alloy and were thought to be relevant to phase formation and hardness within HEAs. Feature selection is critical in ML studies in materials science to produce meaningful and interpretable predictions. Hence, ten independent features were selected from the relevant literature, detailed in Table 1, calculated through an assortment of Hume-Rothery rules, Gibbs free energy rules, and valence electron criteria. These features have previously been shown to be useful in identifying key areas of compositional space. For example, Guo et al.[52] demonstrated that valence electron concentration effectively discriminated between FCC and BCC phase formation within HEAs. As CALPHAD is being used as a down-selection method, features derived from CALPHAD calculations, commonly used in ML HEA studies, have been excluded in this work, e.g., the solidus and liquidus temperatures of the alloys.[32] A subset of these features was trialed in different combinations to assess their impact on the predictive capability of the models. It was found that utilizing the full feature space detailed in Table 1, resulted in improved prediction accuracy, while importantly, the correlation matrix (discussed later in Section 4) did not reveal excessive correlations between features that may result in overfitting. Consequently, in the models trained herein, all features were retained. In subsequent sections, the importance and correlation between the individual features are discussed.\n\n### Machine Learning Model Selection\n\nThe critical factor in the model selection process is the availability of data[53] and it is common practice for ML studies to compare multiple models to find the optimal model for the available data.[47, 48] For example, Bundela and Rahul[54] investigated the performance of a number of different ML models for the prediction of mechanical properties on the same database, produced by Gorsse et al.[44] utilized in this study. Concluding that an artificial neural network performs well on the experimental data, but ultimately finding that an XGBoost model performs best. In this study, an RF architecture was ultimately selected because it was found to outperform the XGBoost proposed by Bundela and Rahul, in addition to other models, such as simple linear regression and support vector machines, on the chosen datasets.\n\nThis is likely caused by the different methodology of data cleaning and the chosen input features used in this study. Deep learning and neural networks were not selected, due to the limited number of data available being unsuitable for training and validation of these model architectures [52], and their increased complexity compared to RF for little gain in prediction accuracy [54]. The RF was also chosen for its ease of construction, interpretability [53] ability to perform both classification and regression tasks [54], and its previous successful application in material science studies [55, 13, 14]. The capability to perform both classification and regression tasks was crucial to both classifying phase formation of compositions and predicting alloy hardness through regression analysis. RF combines many individual and uncorrelated decision trees that are constructed and run in parallel with no interaction. Each tree generates an output for the prediction. In regression analysis, the vote of each decision tree is averaged to produce the final prediction of the model. In contrast, for classification analysis, the majority vote from all the decision trees is taken as the final output of the model [54, 24]. The RF model used in this study was created using the scikit-learn ML toolkit in Python [59]. Firefold shuffled cross-validation was also implemented to enable optimization of model hyperparameters and provide a greater insight into model performance. Optimizing model hyperparameters to, for example, control the growth of trees in the forest is particularly useful for small databases to minimize the likelihood of overfitting. Additionally, a certainty metric for RF predictions, proposed by Kaufmann et al. [31] was employed to provide further insight into model prediction confidence.\n\n### Random Forest Hyperparameters\n\nHyperparameters are settings that control the learning process of the ML model during training. These parameters can be finely tuned and optimized to improve the performance of the model. Each type of ML algorithm has different hyperparameters that impact the learning process in different ways. In the case of RFs, for example, these include the number of decision trees in the forest, how deep each tree is, and the maximum number of features considered when performing a split inside the tree. To determine the optimal hyperparameters, a randomized hyperparameter fivefold cross-validation grid search was performed across a range of values. The algorithm selects a new combination of hyperparameter values on each iteration to construct the RF. To evaluate the random hyperparameter search, the best model produced by the random search was compared to the base model to see if there was an improvement in performance. If there was no improvement in performance the range of hyperparameters was adjusted and the random grid search run again. When a satisfactory improvement in performance was found, then these hyperparameters were implemented into the final ML algorithm.\n\n### Model Training Process\n\nThree individual RF models were produced and trained from the two available databases. From the Gorsse et al. [14] database, a classification model for phase prediction and a regression model for hardness prediction were developed, denoted model X and model Y, respectively. From the Machaka et al. [49] database, another classification model for phase prediction was developed, denoted model Z. Based on the information available in both databases, model X considers 15 different phase outcomes whereas model Z considers 7 different phase outcomes, detailed in Table S1, Supporting Information [14, 40]. Unlike the majority of phase prediction models presented in the literature that only consider binary or tertiary classification tasks, the models trained\n\n\\begin{table}\n\\begin{tabular}{l c c c} Parameter symbol [FOOTNOTE:]Footnote : Quantities denoted with a bar indicate that it is the average value, while quantities with an \\(i\\) indicate the \\(7\\)th element. \\(c\\) represents the atomic fraction of the element, \\(r\\) denotes the atomic radii, and \\(E\\) represents the Youngs modulus. \\(R\\) is the molar gas constant. For \\(r\\), \\(a\\), denotes the solid angles around the largest and smallest atoms, represented by subscript \\(L\\) and \\(S\\), respectively. In \\(\\Delta t_{\\text{max}}\\), \\(\\Omega_{S}\\) is the enthalpy coefficient for elements \\(i\\) and \\(j\\), respectively.[ENDFOOTNOTE] & Parameter name & Equation & References \\\\ \\hline \\(\\delta\\) & Atomic size difference & \\(\\delta=100\\sqrt{\\sum_{i=1}^{n}\\epsilon(1-\\frac{c}{T})^{2}}\\) & [72, 73, 76] \\\\ \\(r\\) & Atomic packing parameter & \\(\\gamma=\\alpha_{S}/\\alpha_{T}\\) & [75] \\\\  & & \\(\\alpha_{S}=1-\\sqrt{\\frac{c(T-T^{2})}{\\alpha_{T}/\\alpha_{T}}}\\) & [77] \\\\ \\(\\Delta r\\) & Electromagentivity difference & \\(\\Delta r=\\sqrt{\\sum_{i=1}^{n}\\epsilon(\\Delta r-T)^{2}}\\) & [77] \\\\ VFC & Valence electron concentration & VFC \\(=\\sum_{i=1}^{n}\\epsilon(\\text{VFC})_{i}\\) & [51, 78] \\\\ \\(e/a\\) & Number of linear electrons per atom & \\(\\hat{\\varepsilon}=\\sum_{i=1}^{n}\\epsilon(\\hat{\\Omega}_{i})\\) & [25, 51, 78] \\\\ \\(\\Delta t_{\\text{max}}\\) & Enthaly of mixing & \\(\\Delta t_{\\text{max}}=\\sum_{i=1}^{n}\\epsilon(\\hat{\\Omega}_{i})\\) & [79, 80, 81] \\\\ \\(\\Delta S_{\\text{max}}\\) & Entropy of mixing & \\(\\Delta S_{\\text{min}}=-R\\sum_{i=1}^{n}\\epsilon(\\hat{\\Omega}_{i})\\) & [7, 80] \\\\ \\(T_{m}\\) & Weighted melting temperature & \\(T_{m}=\\sum_{i=1}^{n}\\epsilon(T_{m})_{i}\\) & [82] \\\\ \\(\\Omega\\) & Yang parameter & \\(\\Omega=\\frac{T_{m}S_{m}}{\\sqrt{\\sum_{i=1}^{n}\\epsilon(1-\\frac{c}{T})^{2}}}\\) & [82] \\\\ \\(\\Delta r\\) & Young’s modulus asymmetry & \\(\\varepsilon=\\sqrt{\\sum_{i=1}^{n}\\epsilon\\left(1-\\frac{c}{T}\\right)^{2}}\\) & [83] \\\\ \\end{tabular}\n\\end{table}\nTable 1: A table of the features used to mathematically describe the alloy compositions, enabling the ML models to make predictions on their phase formation and hardness.\n\nherein have an increased number of outcomes. It is therefore anticipated that the classification models from this work will result in lower prediction accuracies in comparison to binary and tertiary classification tasks [29, 31, 45, 54, 56, 60].\n\nCross-validation was implemented to provide an accurate assessment of the performance of the RF models. The data were split into five equal and randomized folds, with each being used once as the testing set while the others were used for training. After cross-validation and assessments of the model's performance, the models were retrained using the whole database, enabling training on the maximum amount of data available to improve their performance. A measure of the prediction confidence, developed by Kaufmann et al. [31] for RF classification algorithms, was applied in this study. The model's confidence in its prediction was calculated as the ratio of the number of decision trees inside the forest voted for the final phase prediction of that composition, against the total number of trees. Hence, the more trees that vote for a phase, the more confident the model is in its prediction of that phase. This confidence measure was not applied to the hardness prediction model Y, as it is not suitable for an RF regression model. Instead, the coefficients of determination, \\(R^{2}\\) and the RMSE, were used to assess the error and confidence in the regression model.\n\n### Generation of Virtual Candidate Search Space\n\nAfter the model training process, a virtual candidate search space was created for the ML to make predictions on the phase formation and hardness of a large number of HEA compositions. Elements were selected that were included in the chosen databases based on domain knowledge to promote FCC phase formation, as defined in the design criteria. Additionally, the elements were selected for minimizing cost and maximizing raw element abundance, not considering recycled sources. The process for generating the candidate search space followed the sequence described below: 1) Definition of key elements relevant to the case study and design application: Fe, Ni, Co, Al, Ti, W, Cr, Mn, Hf, Nd, Mo, and Ta. 2) Consideration of every possible equiatomic permutation of these elements in a five-element system without repetition, in this case for a total of 792 compositions. 3) Creation of every possible compositional permutation of these five element systems between a minimum of 5 at% and a maximum of 45 at% elemental weighting, with a granularity of 5 at%, yielding a total of 2 238 193 compositions.\n\n## 4 Machine Learning Performance, Outputs, and Discussion\n\nFivefold cross-validation assessments of the performance of the RF models after the training process were conducted to understand how the model would perform on the training data. Accuracy, precision, recall, and \\(F_{1}\\) scores were all calculated to assess model performance for the phase classification task, the results of which are shown in Table 2[61, 62]. Performance scores \\(>\\)78% and \\(>\\)82% for classification models X and Z, respectively, indicate that the models successfully predict phase formation on the validation data from the dataset. Furthermore, it suggests that the models do not suffer from overfitting, enabling potential generalizability to predictions on unseen compositions within the virtual candidate search space. In contrast to the phase classification models, numerically and physically meaningful values for accuracy of the hardness regression model (Model Y) could be obtained. The MAE, \\(R^{2}\\), and RMSE were utilized to evaluate the error between the predicted value produced by model Y and known value of hardness from the database. The results of these performance assessments, determined for the hardness predictions of model Y are also displayed in Table 2. These indicate a good fit and correlation between the predictions and experimental data within the databases used for training the models.\n\nof points lie close to the diagonal red solid line that represents perfect agreement, again showing the good correlation between predicted and database hardness values. A linear fit of the predicted hardness against the database hardness is denoted by the black solid line. The closer the diagonal red solid line is to the linear fit of the black solid line, the lower the systematic error of the model.\n\nThe Pearson's correlation coefficient (PCC) of the input features, denoted by \\(r\\), Equation (2), was also calculated for both databases to measure the linear correlation between any two of the features, highlighting any interdependencies\n\n\\[r=\\frac{\\sum_{i=1}^{n}(x_{i}-\\overline{x})(y_{i}-\\overline{y})}{\\sqrt{\\sum_{i= 1}^{n}(x_{i}-\\overline{x})^{2}}\\sqrt{\\sum_{i=1}^{n}(y_{i}-\\overline{y})^{2}}} \\tag{2}\\]\n\nwhere \\(x\\) and \\(y\\) denote two of the features and \\(\\overline{x}\\) and \\(\\overline{y}\\) represent the mean of the two features, respectively. PCC values can range from \\(+1\\) to \\(-1\\), with positive values indicating a positive relationship between the variables and negative values indicating a negative relationship. Commonly in correlation analysis, if the correlation coefficient between two features is \\(>\\)0.80, then this is considered a very strong correlation [63, 64] and the feature that ranks the highest in the feature importance is retained, while the other feature is eliminated from the model [65, 66]. However, ML studies in the HEA space typically allow for much stronger feature correlations before eliminating features from the model, such as \\(r\\)\\(>\\) 0.95 [21, 26, 33, 45, 46]. When analyzing the results of correlation analysis between features utilized by the ML model, contact is critical to understand the extent and impact of the correlation. Domain knowledge is useful in helping understand and interpret correlations in a more meaningful way. It can be seen from the PCC matrices in **Figure 3**, that the features used in this study are correlated to varying degrees. In both correlation matrices, the pair of features having one of the highest degrees of correlation are \\(\\delta\\) and \\(\\gamma\\) with values of 0.77 and 0.62 for models based on the databases by Gorsse et al. [44] and Machaka et al. [49] respectively. This was anticipated as both features relate to the distribution of atomic radii of the alloys' constituent elements. However, based on the levels of correlation observed it was determined suitable to retain both features for training of the ML model. \\(\\Delta\\chi\\) and \\(\\Delta\\varepsilon\\) were the next strongest positively correlated features from the two databases, 0.72 and 0.65 across the Gorsse et al. [44] and Machaka et al. [49] databases, respectively. Interestingly, comparing across the literature, a broad range of correlation values have been reported for these features. Huang et al. [45] reported a value comparable to this study of 0.71 for the correlation of \\(\\Delta\\chi\\) and bulk modulus asymmetry (a similar correlation would be expected for \\(\\Delta\\varepsilon\\)[67]). In contrast, Chen et al. [64] reported a correlation value of 0.11, although this may be the result of considering a single HEA system. The strongest negative correlation occurs between valence electron concentration (VEC) and \\(\\Delta\\varepsilon\\), \\(-\\)0.68 and \\(-\\)0.62 across the Gorsse et al. [44] and Machaka et al. [49] databases, respectively. This observed correlation agrees with the negative correlation of \\(-\\)0.71 reported by Chen et al. [64]. The negative correlation observed between VEC and \\(\\epsilon\\)/\\(\\alpha\\) was unexpected as both features describe electronic structure and hence, they would be expected to positively correlate. However, both VEC [51] and \\(\\epsilon\\)/\\(\\alpha\\)[25] have been shown to be effective in the prediction of HEA phase formation, thus they were both retained within the models. The feature correlations observed across the two databases are comparable and hence it would be expected that the features would have similar impacts on prediction of phase formation and hardness across the ML models constructed from these databases.\n\nTo interpret and understand the ML, permutation feature importance was utilized. Permutation feature importance is a global ML interpretation technique that describes the average behavior of the model to show general mechanisms and trends, by measuring the increase in prediction error as the model's parameters are permutated. A feature is considered important if randomly shuffling its values increases the error of the model, as in this case, the model relies upon this feature to make its predictions. In contrast, a feature is considered unimportant if shuffling its values does not significantly impact the model's error [34]. Hence, permutation importance analysis highlights the key features influencing phase and hardness predictions in HEAs.\n\nThe results of the permutation feature importance assessment in this study are shown in **Figure 4**. VEC is considered the most important feature for phase prediction in both classification models, X and Z. In addition, VEC is also an important feature in model Y, for hardness predictions, an encouraging result due to the dependence of hardness on underlying phase constitution. This agrees strongly with literature, with several reports finding VEC to be the most important feature in the determination of phase formation in HEAs [35, 38, 46, 28]. Furthermore, the importance of VEC in phase formation has previously been established by Hume-Rothery, who found that similar crystal structures are formed if the VEC of two intermetallic compounds are comparable [69, 10]. In addition, Guo et al. [51] demonstrated that higher values of VEC lead to FCC phase formation and lower values of VEC lead to BCC phase formation. Notably, model X is found to be more strongly affected by VEC than model Z, likely reflecting the experimental databases used. The Gorsse et al. [44] database used for model X includes more complete phase information, whereas the Machaka et al. [49] database used for model Z, often groups intermetallic phases and HCP solid solutions together into simpler phase classes, as described in Table S1, Supporting Information. Furthermore, the relative size of the databases, with the Machaka et al [49] database being significantly larger than the Gorsse et al. [44] database, will likely reduce the individual feature dependence of the derived models. In contrast to VEC, \\(\\Delta\\chi\\), is found to have little impact on the predictions of all three models, shown last in all permutation feature importance assessments. For the hardness predictions of regression model Y, \\(\\Delta\\varepsilon\\) is shown to be significantly the most important feature, but in contrast, it is not considered important by the two-phase classification models X and Z.\n\nSHAP is a local interpretation method that can be used to explain individual predictions by determining the contribution of each feature [70]. SHAP summary plots combine feature importance and feature effects [24] and are shown in **Figure 5** for each model, respectively. Each point in the plots represents a SHAP value for a feature and prediction for an individual composition. Overlapping points are ittered in the \\(y\\)-axis to provide an illustration of the distribution of SHAP values per feature. A positive/negative SHAP value indicates a positive/negative impact on model predictions, associated with the likelihood to predict FCC phase formation and values of hardness, both for the classification and regression tasks, respectively [142, 71]. Hence, the wider the horizontal distribution of points, the greater the influence of that feature on the models' predictions. Additionally, features are ordered on the \\(y\\)-axis by their importance for predictions. The color of the points denotes the value of the feature in question. Red indicates a larger feature value, while blue indicates a smaller feature value [24].\n\nEquivalent conclusions to the PCC and permutation importance in Figure 3 and 4, respectively, can be drawn from the SHAP summary plots shown in Figure 5. All three models place\n\nFigure 3.— A correlation map between the ten features used as mathematical descriptors for the ML models. The value in the grid shows the PCC between the different features and the color intensity is proportional to the magnitude of the PCCs. A) From the Gorsse et al. [144] database; B) From the Machaka et al. [49] database.\n\na high importance on VEC. Large VEC values produce large positive SHAP values for FCC phase prediction and large negative SHAP values for hardness prediction. Indicating that for compositions with a larger VEC value, the ML is more likely to predict FCC phase formation and lower values of hardness. This shows strong agreement with the formation of FCC phases at higher values of VEC, resulting in lower values of hardness, as harder BCC and intermetallic phases are less likely to form.[22, 15, 54, 72]\n\nFigure 4: Plots of the permutation importance of the ten features used as mathematical descriptors for the ML models. A) From classification model X; B) From classification model Z; C) From regression model Y.\n\nFigure 5: SHAP value distribution plots of different compositions, showing the importance and effects of different features. A) From model X on FCC phase formation; B) From model Z on FCC phase formation; C) From model Y on hardness predictions.\n\n## 4 Advanced Science News\n\nThis is encouraging that the ML models are producing outputs with sensible dependencies based on empirical physics understanding. In contrast, \\(\\Delta S_{\\text{mix}}\\) is placed toward the bottom of all three SHAP summary plots. Model Y places low importance on the configurational entropy in the prediction of alloy hardness, producing a 0.30% decrease in model performance under permutation analysis, as shown in Figure 4b. In contrast, for models X and Z, it is shown to have a slightly more significant and also comparable impact. A 2.47% and 2.94% decrease in model performance under permutation analysis are shown in Figure 4a,c. Models X and Z predicting phase formation are trained from different databases but perform comparably. Both databases[44, 45] used in this study have comparable numbers of compositions forming solid solutions, around 60%. In this case, the calculation of the configurational or mixing entropy is based upon ideal randomly distributed solid solutions and is often not representative of the total entropy. Excess vibrational, magnetic moment, and electronic effect terms can contribute significantly to the total entropy and impact the role of entropy on phase selection within HEAs.[79]\n\n### High-throughput Machine Learning Predictions and CALPHAD Analysis\n\nAll three RF models were used to make high-throughput predictions on the phase and hardness of a series of unseen HEA compositions, with the goal of downselecting a small sample of compositions for experimental testing, to further validate and refine the models. The results of these high-throughput predictions were collated and subsequently abridged. To satisfy the design criteria outlined for the case study considered herein, CALPHAD analysis was employed to enable further downselection of alloys for experimental fabrication from the ML outputs.\n\nTo meet the design criteria of the chosen case study, as a first step, all compositions where classification models X and Z did not both predict FCC phase formation were eliminated. A total of 24 613 HEA compositions were predicted to form an FCC phase by both models X and Z. CALPHAD calculations of the equilibrium phase formation of the remaining HEA compositions at 1000 C were also obtained using the TCNi8 database within the Thermo-Calc software package and automated using TQ-Fortran. CALPHAD calculations are known to be less accurate at \"low\" temperatures, predicting often kinetically inhibited or unrealistic intermetallic phase formation. In addition, calculations at temperatures over 1000 C may eliminate a number of compositions if the solidus temperature of the material is low. 1000 C was therefore chosen as a compromise temperature between accuracy of CALPHAD predictions and practicability of the chosen case study, with 1000 C representing a sensible operating limit. TCNi8 was utilized in this study as an FCC crystal structure was targeted and hence it was considered to be the most relevant database, providing more holistic information for the systems under consideration. Compositions were sorted by FCC phase fraction formation and subsequently, ML-predicted hardness. The twelve hardest FCC-predicted compositions (by all models and CALPHAD) are shown in Table 3. Due to compositional similarities, the compositions highlighted in bold in Table 3 and labeled A-C, were chosen as the most interesting to experimentally investigate. In addition, the three highest hardness alloys, as shown in Table 4 and labeled D-F, irrespective of CALPHAD-predicted phase formation, were selected for experimental testing to enable an improved understanding of the performance of the regression model constructed. It is worth noting that the higher hardness as predicted by model Y in Table 4 is likely due to the alloys not being FCC based, as indicated by the CALPHAD predictions, despite being projected to form a single-phase solid solution FCC by ML models X and Z.\n\nThis methodology, first applying ML and subsequently downselecting using CALPHAD, has a number of key benefits[42]. Performing CALPHAD analysis of over 2 million compositions considered by the ML in this study is computationally impractical and time intensive. Initial application of ML dramatically reduces the number of compositions that need to be considered and hence the computational time. Applying CALPHAD data as an input to the ML model raises the issue of training the model on calculated data as opposed to experimentally determined data. While CALPHAD offers a remarkable tool for alloy design, it is well documented that calculation accuracies are lower for compositions away from established alloy systems. This is due to the construction of the databases being built from binary and ternary systems as well as the lack of full characterization of such systems across the relevant compositional space. Furthermore, CALPHAD provides calculations of equilibrium phases only. Therefore, caution is necessary as for manufacturing purposes reaching equilibrium phases can be unrealistic. Hence, CALPHAD can bias the results against compositions that are kinetically sluggish, but would otherwise be suitable for application. While this latter point is not applicable to the chosen case study, the use of ML is envisaged to progressively evolve for increasingly more complicated case studies, where equilibrium and indeed the use of CALPHAD may in fact be inadvisable. Therefore, in this case study CALPHAD was used simply as a downselection tool only.\n\n\\begin{table}\n\\begin{tabular}{l l c c c} \\hline \\hline Sample & Alloy & ML confidence & ML confidence & ML hardness \\\\ index & model X [Y] & model [Z] [Y] & model Y [H] \\\\ \\hline\n**A** & **Cr\\({}_{20}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{45}\\)H\\({}_{5}\\)** & 47.0 & 41.5 & 483 \\\\ – & Cr\\({}_{20}\\)Mn\\({}_{35}\\)Fe\\({}_{30}\\)Ni\\({}_{45}\\)H\\({}_{5}\\) & 43.0 & 42.5 & 479 \\\\\n**B** & **Ti\\({}_{25}\\)Cr\\({}_{20}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{5}\\)** & 57.0 & 37.4 & 476 \\\\ – & Ti\\({}_{20}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{5}\\) & 43.0 & 38.6 & 474 \\\\ – & Ti\\({}_{20}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{5}\\) & 39.0 & 39.5 & 473 \\\\ – & Cr\\({}_{35}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{5}\\)H\\({}_{5}\\) & 49.0 & 41.6 & 471 \\\\ – & Cr\\({}_{35}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{5}\\)H\\({}_{5}\\) & 48.0 & 45.5 & 471 \\\\ – & Cr\\({}_{35}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{5}\\)H\\({}_{5}\\) & 48.0 & 41.6 & 468 \\\\ – & Cr\\({}_{20}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{5}\\)H\\({}_{5}\\) & 45.0 & 40.5 & 461 \\\\ – & Cr\\({}_{30}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{5}\\)H\\({}_{5}\\) & 50.0 & 46.4 & 460 \\\\ – & Cr\\({}_{30}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{5}\\)H\\({}_{5}\\) & 50.0 & 46.9 & 452 \\\\\n**C** & **Ti\\({}_{20}\\)Mn\\({}_{25}\\)Fe\\({}_{30}\\)Ni\\({}_{5}\\)Mo\\({}_{5}\\)** & 32.0 & 37.5 & 451 \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 3: Table of the hardest alloy compositions according to model Y, also predicted to display single-phase FCC formation by models X and Z, and CALPHAD equilibrium phase fraction calculations. The compositions highlighted in bold and labeled A–C were chosen as the most interesting to experimentally investigate.\n\nThus, the ML architecture can predict compositional hardness from experimental data, save significant time over the CALPHAD approach and can be quickly updated with new data, while providing reproducible and accurate predictions. Utilizing CALPHAD as a postprocessing tool to interrogate a small subset of the ML predictions provides further detailed microstructural information for alloy downselection, but importantly retains separation and allows critical interrogation of predictions. In the absence of further experimental data, CALPHAD provides a robust tool to enhance the phase predictions obtained from the ML models to target experimentation into both areas of agreement where technological benefits can be derived, as well as areas of discrepancies where improved experimental data can be of great importance.\n\n## 5 Experimental Section\n\nTo validate the ML model and assess the suitability for the design use case of the downsselected alloy compositions in Tables 3 and 4, the HEA systems were synthesized via arc melting in an inert Ar atmosphere. Button ingots of 30 g were produced for each composition from high purity (>99.5% by mass) bulk raw elements. To achieve alloy melt homogeneity, the ingot was inverted and remelted for a minimum of seven melting cycles with the same arc current intensity. The ingots were melted and solidified in a water-cooled Cu crucible, resulting in faster cooling rates than other typical casting methodologies. Due to the low evaporation point of Mn in comparison to the melting point of some of the refractory elements included in the alloy compositions, extra Mn (in the range 10-20%) was added to account for the anticipated losses during the manufacturing process. Differential scanning calorimetry (DSC) measurements of the as-cast alloys were conducted using a TA Instruments SDT Q600, to determine suitable temperatures for homogenization heat treatments. Samples were heated at a rate of 20 \\({}^{\\circ}\\)C min\\({}^{-1}\\) to a maximum temperature of 1450 \\({}^{\\circ}\\)C and held at this temperature for two minutes before a controlled cool at the same rate to 400 \\({}^{\\circ}\\)C, holding for two minutes. Two cycles of this heating and cooling regime were performed. Plots of the DSC measurements can be found in Figure S1, Supporting Information. Two separate homogenization regimes were identified based upon the DSC data, 1000 \\({}^{\\circ}\\)C and 1150 \\({}^{\\circ}\\)C for 48 h, for alloys A-C and D-F, respectively, followed by air cooling. These temperatures were selected to be as close to the solidus temperature of the alloys as possible without the risk of incipient melting.\n\nSpecimens from each as-cast and heat-treated condition were prepared for microstructural characterization by grinding with P400 - P2500 grades of SiC paper and polishing to a 1 \\(\\upmu\\)m finish using a diamond. Imaging and compositional characterization were performed using scanning electron microscopy (SEM) coupled with energy-dispersive X-ray spectroscopy (EDX) on an FEI Inspect F50 and JEOL 7900 F operated at 20 keV and equipped with a Bruker XFlash 6 solid-state EDX detector. Additional phase characterization of each as-cast and heat-treated condition in bulk alloy form was performed using X-ray diffraction (XRD) on a Panalytical Aeris diffractometer using Ni-filtered Cu Ka radiation. Patterns were recorded in the 10\\({}^{\\circ}<2\\theta\\) \\(<100^{\\circ}\\) range at 0.02\\({}^{\\circ}\\) increments and were analyzed using the full-pattern Pawley fitting procedure[73] in TOPAS-academic. Characterization of alloy hardness in both as-cast and homogenized conditions was performed using microindentation hardness on a Durascan 70 G5 by applying a 1 kgf load with a 15 s dwell time for a series of ten indentations, in accordance with ASTM E384.[74]\n\n## 6 Experimental Results and Discussion\n\nThe bulk elemental compositions following the arc-melting fabrication procedure were confirmed by averaging large-area EDX scans. The results of this and comparison to the nominal compositions from the ML virtual candidate search space are detailed in Table 5. In the majority of cases, the bulk elemental compositions are within \\(\\pm\\)4% of the target concentrations for each element. However, for some elements, in particular for the Mo, Cr, and Mn, there is often a significant difference between the target and the fabricated compositions. This is due to the higher melting points of Mo and Cr and the low evaporation temperature of Mn compared to the other elements in the alloy systems leading to evaporation of Mn and possible lack of melting of Mo and Cr. The phase formation in the experimentally fabricated compositions was also evaluated through both the ML and CALPHAD and was found to produce the same predictions as the nominal compositions. Therefore, it was deemed suitable to use these alloy compositions for further experimental evaluation to assess the fidelity of the models utilized.\n\n### Microstructural Analysis of As-cast and Homogenized Material\n\nMicrostructural analysis of the alloy compositions was performed by SEM to determine the phases present and their relative chemistry. Backscattered electron (BSE) micrographs of the alloy compositions in the as-cast state are presented in Figure 6. Following the rapid solidification from the arc-melting process, all alloys exhibited a large-grained microstructure with high contrast due to both crystal orientation and compositional variation. SEM-based EDX maps, Figure S2-S13, Supporting\n\n\\begin{table}\n\\begin{tabular}{l c c c c c c} Sample index & Alloy & ML confidence & ML confidence & ML hardness & CALPHAD phase & CALPHAD phase fraction \\\\  & model X [Y] & model Z [Y] & model Y [H] & model Y [H] & & \\\\ \\hline D & Al\\({}_{\\text{Na}}\\)Ti\\({}_{\\text{Ca}}\\)Po\\({}_{\\text{Ni}}\\)Nb\\({}_{\\text{Ni}}\\)Nb\\({}_{\\text{Ni}}\\) & 26.0 & 26.0 & 587 & C14 Lives, BCC, and FCC & 42.5\\%, 41.0\\% and 16.5\\% \\\\ E & Al\\({}_{\\text{Na}}\\)Ti\\({}_{\\text{Zr}}\\)Fe\\({}_{\\text{Ni}}\\)Nb\\({}_{\\text{Ni}}\\)Nb\\({}_{\\text{Si}}\\) & 40.0 & 39.0 & 584 & BCC and C14 Lives & 60.8\\% and 39.2\\% \\\\ F & Al\\({}_{\\text{Na}}\\)Ti\\({}_{\\text{Zr}}\\)Cr\\({}_{\\text{Zr}}\\)Fe\\({}_{\\text{Ni}}\\)Nb\\({}_{\\text{Si}}\\) & 47.0 & 41.0 & 583 & BCC & 100\\% \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 4: Table of the hardest alloy compositions according to model Y, also predicted to display single-phase FCC formation by models X and Z, irrespective of CALPHAD equilibrium phase fraction calculations.\n\nFigure 6: Following the rapid solidification from the arc-melting process, all alloys exhibited a large-grained microstructure with high contrast due to both crystal orientation and compositional variation. SEM-based EDX maps, Figure S2-S13, Supporting\n\nInformation, highlight clear elemental segregation within the grains, indicating dendritic solidification in all alloys. In all cases, no evidence of single-phase solid-solution formation was found, with all alloys displaying a multiphase microstructure.\n\nTo remove or mitigate the microsegregation observed due to the rapid solidification of the casting process, alloys A-C were homogenized at 1000 \\({}^{\\circ}\\)C for 48 h and alloys D-F were homogenized at 1150 \\({}^{\\circ}\\)C for 48 h, followed by air cooling. BSE micrographs of the alloy compositions in the homogenized condition are presented in **Figure 7**. The SEM-based EDX maps, Figures S2-S13, Supporting Information, highlight clear elemental segregation within the grains between the matrix and precipitating phases. As with the as-cast state, there is no evidence of single-phase FCC solid solution formation in any of the fabricated compositions. However, there is a clear change in microstructure due to the homogenization process for all alloys. In all cases, the homogenization process was sufficient to remove segregation within the solidifying phases and ensure chemical equilibration of the composition of each phase. The skeletal interdendritic solidification in alloys in the as-cast state was also\n\n\\begin{table}\n\\begin{tabular}{l c c c c c} Sample index\\({}^{\\text{a)}}\\) & Nominal alloy composition & Experimental alloy composition & ML confidence & ML confidence & ML hardness \\\\  & & & model X [g] & model Z [g] & model Y [h] \\\\ A & Cr\\({}_{23}\\)Mn\\({}_{23}\\)Fe\\({}_{3}\\)Ni\\({}_{35}\\)H\\({}_{5}\\) & Cr\\({}_{23}\\)Mn\\({}_{13}\\)Fe\\({}_{0.23}\\)Ni\\({}_{4.8}\\)H\\({}_{5.8}\\) & 42.0 & 41.5 & 477 \\\\ B & Ti\\({}_{23}\\)Cr\\({}_{3}\\)Mn\\({}_{23}\\)Fe\\({}_{49}\\)Ni\\({}_{35}\\) & Ti\\({}_{140}\\)Cr\\({}_{49}\\)Mn\\({}_{23}\\)Fe\\({}_{49}\\)Ni\\({}_{35}\\) & 60.0 & 43.5 & 478 \\\\ C & Ti\\({}_{23}\\)Mn\\({}_{23}\\)Fe\\({}_{49}\\)Ni\\({}_{35}\\)Mo\\({}_{15}\\) & Ti\\({}_{23}\\)Mn\\({}_{23}\\)Fe\\({}_{13}\\)Ni\\({}_{35}\\)Mo\\({}_{23.3}\\) & 37.0 & 48.0 & 429 \\\\ D & Al\\({}_{23}\\)Ti\\({}_{3}\\)Co\\({}_{23}\\)Ni\\({}_{35}\\)Nb\\({}_{15}\\) & Al\\({}_{185}\\)Ti\\({}_{23}\\)Co\\({}_{23}\\)Ni\\({}_{10}\\)Nb\\({}_{15.8}\\) & 24.9 & 30.9 & 586 \\\\ E & Al\\({}_{23}\\)Ti\\({}_{23}\\)Fe\\({}_{49}\\)Ni\\({}_{35}\\)Mo\\({}_{5}\\) & Al\\({}_{185}\\)Ti\\({}_{23}\\)Fe\\({}_{13}\\)Ni\\({}_{4.8}\\)Mo\\({}_{4.8}\\) & 40.7 & 40.0 & 586 \\\\ F & Al\\({}_{23}\\)Ti\\({}_{23}\\)Cr\\({}_{3}\\)Fe\\({}_{49}\\)Ni\\({}_{35}\\) & Al\\({}_{23}\\)Cr\\({}_{13}\\)Cr\\({}_{13}\\)Fe\\({}_{23}\\)Ni\\({}_{13.3}\\) & 42.7 & 41.0 & 590 \\\\ \\end{tabular}\n\\end{table}\nTable 5: Table comparing the downselected alloys nominal composition generated as part of the virtual candidate search space to the experimentally fabricated composition.\n\nFigure 6: BSE micrographs of samples in the as-cast state showing two and three-phase dendritic microstructures. Samples are labeled A–F) according to Table 5.\n\nobserved to break down in most cases in the as-homogenized samples in favor of discrete particles. Importantly, all compositions were found to be multiphase even after the homogenization/solution heat treatments.\n\nElemental partitioning information was obtained, and phase identification was performed through EDX and XRD techniques respectively. A summary of the data extracted from the EDX and XRD analysis is given in Table 6. Results presented in Table 6 agree with the SEM observations as discussed above.\n\n### Mechanical Property Analysis of As-cast and Homogenized Material\n\nMechanical properties of the alloys were assessed by microindentation hardness testing, as outlined in Section 5. Results of the microindentation hardness assessments of the alloys in both the as-cast and homogenized condition are presented in Figure 8. In all cases for alloys A-C, predicted to form single-phase FCC solid-solutions by both ML and CALPHAD, but experimentally\n\n\\begin{table}\n\\begin{tabular}{l l l l l} Sample Index & Experimental Alloy Composition & Condition & EDX Analysis & XRD Analysis \\\\ A & C\\({}_{12.3}\\)Mn\\({}_{13.75}\\)Fe\\({}_{12.3}\\)Ni\\({}_{4.5}\\)Hf\\({}_{4.8}\\) & As-Cast & Light Contrast = Ni, Hf rich & FCC +HN\\({}_{3}\\) + BCC \\\\  & & & Dark Contrast = Cr, Fe rich & \\\\  & & Homogenised & Light Contrast = Ni, Hf rich & FCC +HN\\({}_{3}\\) + BCC \\\\  & & & Dark Contrast = Cr, Mn, Fe rich & \\\\ B & Ti\\({}_{10.0}\\)Cr\\({}_{8.6}\\) Mn\\({}_{12.2}\\)Fe\\({}_{12.3}\\)Ni\\({}_{3.8}\\) & As-Cast & Light Contrast = Ni and Ti rich+ & FCC \\\\  & & & Dark Contrast = Cr, Mn, Fe rich & \\\\  & & Homogenised & Dark Contrast = Cr, Mn, Fe, Ti & FCC + BCC + Ni\\({}_{3}\\)Ti \\\\  & & & Light precipitates = Ti rich+ & \\\\ \\end{tabular}\n\\end{table}\nTable 6: Summary of the EDX and XRD analysis of the alloy systems in the as-cast and homogenized state. EDX elemental maps and an example XRD pattern overlaid with a Pawley fit can be found in the Supporting Information.\n\nFigure 7: BSE micrographs of samples in the homogenized state according to the heat treatments outlined in Section 5, showing two and three-phase dendritic microstructures. Samples are labeled A–F) according to Table 5.\n\nobserved to display multiple secondary phases, the ML-predicted hardness is significantly higher than that of both the as-cast and homogenized conditions. However some agreement within the range of uncertainty of the experimentally determined values was observed. Furthermore, a marginal increase in hardness was exhibited from the as-cast to the homogenized state for all three alloys, which was found to be consistent with the observed changes in microstructure. In all cases for alloys D-F, predicted to form single-phase FCC solid solutions by the ML but not CALPHAD, the ML-predicted hardness is observed to be consistent with the observed changes in microstructure. In all cases for alloys D-F, predicted to form single-phase FCC solid solutions by the ML but not CALPHAD, the ML-predicted hardness is observed to be consistent with the observed changes in microstructure.\n\n\\begin{table}\n\\begin{tabular}{l l l l l} \\hline Sample index & Experimental Alloy Composition & Condition & EDX Analysis & XRD Analysis \\\\ C & T\\({}_{\\text{Ta,Ta,Ta,Sr,Ni,Na,Mo}}\\) & As-Cast & Light Contrast = Fe, Mo rich & FCC + Ni\\({}_{\\text{Ti}}\\) + Fe\\({}_{\\text{Mo}}\\) (C14 Laves) \\\\  & & Mid Contrast = Ni, Ti rich & Dark Contrast = Mn, Fe rich & \\\\  & & Homogenised & Light Contrast = Ti, Fe, Mo & FCC + Fe\\({}_{\\text{Mo}}\\) (C14 Laves) \\\\  & & Mid Contrast = Ti rich & Dark Contrast = Ni, Mn & \\\\ D & \\(A_{\\text{Na,Sr,Ti,Co}}\\)\\({}_{\\text{Na,Ni,Nb}}\\)\\({}_{\\text{Na,6}}\\) & As-Cast & Light Contrast = Co, Ni, Nb rich & FCC + TAU (B2) + (CoNi)\\({}_{\\text{Nb}}\\) (C14 Laves) \\\\  & & Mid Contrast = Al, Co, Ni, Nb & Ni (TAU) (B2) \\\\  & & Homogenised & Light Contrast = Co, Nb, Ni & Ni(TAU) (B2) \\\\  & & Dark Contrast = All & Additional Co, Nb rich phase & \\\\ E & \\(A_{\\text{Na,Sr,Ti,Mo}}\\)\\({}_{\\text{Fe,Ni,Mo}}\\)\\({}_{\\text{Na,6}}\\) & As-Cast & Light Contrast = Al, Ti, Mo rich & BCC or B2 + Fe\\({}_{\\text{Ti}}\\)T (C14 Laves) \\\\  & & Dark Contrast = Ni rich & \\\\  & & Homogenised & Light Contrast = Al, Ti, Mo rich & BCC or B2 + Fe\\({}_{\\text{Ti}}\\)T (C14 Laves) + AMO\\({}_{\\text{3}}\\) \\\\  & & Dark Contrast = Al, Ti, Fe, Ni & \\\\  & & Light Precipitates = Mo rich & \\\\ F & \\(A_{\\text{Na,Sr,Ti,Co}}\\)\\({}_{\\text{Ti,Ta,Sr,Ni,3}}\\) & As-Cast & Light Contrast = Al, Ti, Cr & BCC or B2 + TIC/A (C14 Laves) + AIC\\({}_{\\text{2}}\\) \\\\  & & Mid Contrast = Cr rich & \\\\  & & Dark Contrast = Al, Ti rich & \\\\  & & Homogenised & Light Contrast = Al, Ti, Ni rich & BCC or B2 + TIC/A (C14 Laves) + AIC\\({}_{\\text{2}}\\) \\\\  & & Mid Contrast = Cr rich & \\\\  & & Dark Contrast = Al, Ti rich & \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 6: Continued.\n\nFigure 8: A bar chart showing the comparison between the ML-predicted, as-cast, and homogenized microindentation hardness in HV of the six alloys fabricated in this study. Error bars on the predicted hardness values represent the MAE score of the ML regression model. The error bars on the experimentally measured as-cast and homogenized samples represent the standard deviation. The results of the hardness analysis are included in Table S2, Supporting Information.\n\nsignificantly lower than the measured hardness in both the as-cast and homogenized form. In addition, a drop in hardness from the as-cast to the homogenized condition is observed in all cases for these alloys. This apparent reduction could be due to a decrease in the intermetallic phase fraction, or morphological changes as the alloys move from a skeletal structure to discrete particles. However, given that the changes are within the measurement error, this was not investigated further.\n\n## 7 Discussion\n\n### Phase Formation Results Discussion\n\nIt is clear from the experimental results, shown in the BSE micrographs of Figures 6 and 7, that all alloys fabricated did not exhibit a single-phase FCC solid solution, with significant intermetallic formation observed. In fact, only four out of the six compositions exhibit an FCC phase, with the other two showing BCC or B2 matrix phases, clearly demonstrating the inaccuracies in the phase prediction of the ML classification models. Alloy B is observed to form the lowest fraction of intermetallic phase and this is also reflected in the confidence measure of the ML phase predictions, scoring one of the highest FCC phase prediction confidences for the downselected alloys. In general, for the alloy systems downselected to be experimentally analyzed, the confidence measure of the ML phase predictions is low. Pushing toward alloy systems with higher hardness was prioritized over alloy systems likely to produce a single-phase FCC solid-solution, but at a significantly lower hardness. If only compositions predicted to form an FCC phase with confidence\\(>\\)90% in both models X and Z are considered, then of the 24 613 predicted to form an FCC phase by the ML, only 443 would satisfy these criteria and all are predicted by CALPHAD to form a single FCC phase. Furthermore, all 443 of these alloys are a compositional variation of CrMnFeCoNi, as expected, as this system dominates the literature with respect to FCC HEAs. In addition, these compositions have a significantly lower hardness predicted by the ML than those downselected for experimental testing, with an average predicted hardness of only 105 HV. Hence, high-hardness-predicted compositions were prioritized at the expense of high confidence in prediction of a single-phase FCC solid-solution to enable further exploration of HEA compositional space. To assist in alloy downselection, equilibrium phase fraction calculations at 1000 C were performed using CALPHAD and the TCNis database on the compositions predicted to form single-phase FCC by both models X and Z. To draw effective comparisons between the experimentally fabricated alloys and CALPHAD, a long duration exposure at 1000 C and subsequent quench would be required. However, CALPHAD predictions of the same alloy systems as a function of temperature still do not correctly identify the microstructure of the alloy compositions, as described in Table 6, and in most cases predict more complex intermetallic phases that are not observed in the microstructure.\n\n### Hardness Results Discussion\n\nIn general, the ML model incorrectly predicts the observed hardness of the experimentally fabricated alloys, although there is some agreement within errors, as seen in Figure 8. Further scrutiny of Figure 8 reveals distinct patterns and systematic errors in the ML hardness predictions. Alloys A-C are found to be systematically overpredicted. In contrast, for alloys D-F, there is significant underprediction of the hardness compared to the experimentally measured values in both the as-cast and homogenized state. The relative overprediction for alloys A-C compared to the underprediction for alloys D-F is less pronounced. In most cases, taking into consideration the measurement errors, the as-cast and homogenized values are comparable, with the notable exception of alloy B, where the precipitation of Ni\\({}_{3}\\)Ti was found to increase the hardness following homogenization. It is encouraging that the hardness regression model Y captures a difference for the dicre alloy classes from the input features, even though this difference is not captured by the phase model. Nevertheless, the ML did accurately capture the correct trends, with the alloys experimentally determined to display an FCC matrix phase and a secondary intermetallic phase predicted to be a lower hardness than those observed to form BCC and B2 intermetallic-based microstructures.\n\nThe disparity between the ML hardness and measured hardness for alloys D-F occurs because these alloys have been downselected as they were predicted to be the highest hardness alloys to form a single-phase FCC solid solution. As highlighted by the SHAP summary plots in Figure 5 and the permutation feature importance plots in Figure 4, all models place a very high importance on _V_EC and models Y and Z place very high importance on \\(\\Delta e\\). It is expected that \\(\\Delta e\\) would be related to alloy hardness and indeed the SHAP summary plots show that larger values of \\(\\Delta e\\) lead to the model predicting higher values of hardness, Figure 5c. Similarly, in model Z, smaller values of \\(\\Delta e\\) are less likely to predict FCC phase formation, Figure 5b. Furthermore, larger values of V_EC are shown to be more likely to predict FCC phase formation and a lower alloy hardness, Figure 5. Hence, although the classification and regression models are not coupled, the model interpretation metrics discussed in Section 4, reveal that similar emphasis is placed on input features in all models. Therefore, it would be expected that if the classification of ML model predicts a single-phase FCC solid solution, then the parallel regression ML model would also predict a lower hardness. Consequently, as the alloys are not in fact single-phase FCC solid solutions and are in some cases not based on an FCC matrix at all, it would be expected that they would display a significantly higher hardness than that predicted by the ML model. Similarly, the maximum single-phase FCC hardness recorded in the training database was 537 HV. Thus, the ML model would not be expected to extrapolate much beyond this maximum value and certainly would not predict hardness values in excess of 650 HV, such as those exhibited experimentally for alloys D-F. The overprediction observed for alloys A-C is in contrast to the observed microstructure of the alloy. It would be expected that as the model predicts single-phase FCC formation, the presence of secondary intermetallic, seen in the BSE micrographs in Figures 6 and 7, would lead to an increase in alloy hardness over the predictions. Instead, the opposite trend is observed.\n\nThe combined under- and overprediction of the hardness values from the ML regression model Y, compared to the measured values, are believed to be in part due to the experimental HEA mechanical properties database used in this study to train the model. The database contains only 366 values, including values calculated through an empirical relationship with yield strength and imputed values. Of these entries in the database, there are significantly more BCC-forming compositions than FCC, constituting 30.3% and 18.0% of the database respectively. Furthermore, additional analysis of the 66 FCC-labeled compositions revealed that the distribution of hardness values had a mean of 237 HV but a median of 189 HV, a lower quartile of 132 HV, and an upper quartile of 295 HV. In fact, of the 66 FCC-labeled compositions, only 15 were found to be quoted as having a hardness of \\(>\\)400 HV. Additionally, there is an absence of data in the region \\(320<\\mathrm{HV}<420\\). This could potentially indicate that the compositions with higher hardness were mislabeled as single-phase FCC in their respective publications and may have instead exhibited the formation of intermetallic phases.\n\n### Future Research Directions\n\nAs demonstrated and discussed above, the accuracy and confidence of predictions of FCC phase formation by the ML models constructed in this study are lacking. In addition, the hardness model overpredicts in the case of alloys A-C and underpredicts for alloys D-F. Therefore, there is significant potential for improvement of the prediction capability. The simplest and most obvious is the availability of additional high quality and quantity data on the microstructural and mechanical properties of HEAs away from the more commonly studied CrMnFeCoNi compositional space. Following this, if the available data are increased, new complex models such as neural networks can be considered for application to this case study and beyond. In this work, more complex models were not considered due to the small amount of available training data being unsuitable for their construction. Additionally, a smaller compositional granularity can be considered for the virtual candidate search space to minimize the space between compositions and more effectively identify phase transitions from the ML outputs. However, this reduced compositional granularity comes at the cost of more time-intensive computation. Finally, in terms of improving the ML model, the feature space can be refined to include new features that better describe both mechanical properties and phase formation of HEAs.\n\nAdditionally, for the purpose of alloy discovery, to meet the proposed design case in this study, more alloy systems predicted by this model can be selected for experimental analysis. This will determine their suitability to meet the application design criteria and understand the combined ML and CALPHAD predictions. Despite the erroneous ML predictions and the large uncertainties obtained throughout this study, the alloys that have been experimentally evaluated have resulted in compositions that offer promise for the intended application. Further work on these alloys will seek to evaluate carbon reinforcement to both suppress detrimental brittle intermetallic phase formation and improve mechanical properties, enhancing their applicability to the design case.\n\n## 8 Conclusions\n\nThe study aimed to apply a ML methodology to the design of hardmetal matrix compositions for metal-forming tooling applications. This was simplified to two key design criteria, that of high hardness and a single-phase FCC structure as a proxy for ductility and toughness. Hence, a series of RF ML models have been constructed and trained from experimentally determined public databases on the phase and hardness of HEAs to enable predictions of these properties in unexplored compositional spaces. In contrast to the majority of literature, CALPHAD was not used to supplement data or as an input to the models. Instead, CALPHAD was utilized to provide further microstructural investigation following the ML and aid in downselection of suitable alloy compositions for experimental analysis. Six compositions were chosen to be fabricated and their microstructural and mechanical properties were investigated to enable assessment of their ability to meet the design criteria and assess the performance of the ML predictions. These six compositions consisted of the three unique alloy systems predicted to be the hardest single-phase FCC by the ML and the three hardest compositions predicted by both the ML and CALPHAD to form a single-phase FCC microstructure.\n\nInterrogation of the ML models constructed for phase and hardness prediction revealed a strong dependency on features such as VEC and _A_e influencing the prediction outcomes, despite the models not being coupled. This was an encouraging result, indicating that the hardness regression model could correctly capture microstructural parameters in subsequent predictions. This was further reinforced by the experimental results obtained. Although none of the alloys that were experimentally evaluated comprised a single-phase FCC structure, three alloys demonstrated an FCC matrix with the remaining alloys relying primarily on a BCC or B2 phase acting as the matrix. This apparent microstructural change was captured by the hardness model that was found to systematically overpredict the hardness of the FCC-matrix alloys and underpredict the BCC/B2-matrix alloys.\n\nIn contrast to the satisfactory outcomes of the hardness regression model, the phase prediction classification models were found to be inaccurate compared to the experimental microstructural assessments. The discrepancy between experiment and prediction was believed to be primarily due to the databases used in the ML training. The imbalanced data contained within the databases, coupled with the bias of FCC compositions toward the CrMnFeCoNi system, resulted in predictions herein having reduced confidence indicators when downselected with increased hardness being prioritized.\n\nHowever, despite the experimental and predicted discrepancies, the methodology employed identified compositions that are suitable for further experimental evaluation toward the intended use case. Furthermore, the construction of the models and their rigorous analysis have revealed several areas for improvement for both the ML architectures as well as highlighting the need for reliable, extensive, and expansive databases.\n\n## Supporting Information\n\nSupporting Information is available from the Wiley Online Library or from the author.\n\n## Acknowledgements\n\nThis work was supported by Oerlikon AM Europe GmbH (website: [https://www.oerlikon.com/am/en/](https://www.oerlikon.com/am/en/)), Engineering and Physical Sciences Research\n\n## Appendix A Advanced Science News\n\n1 Cumul UK (EP/5022635/1) (website: [https://www.ukir.org/ecuncils/epscr/](https://www.ukir.org/ecuncils/epscr/)), and Science Foundation Ireland (18/EP/SRC-CD7/3584/ (website: [https://www.sif.ie/](https://www.sif.ie/)). Access to experimental facilities and expertise was made possible through the Royce Institute (EP/RO0661X/1, EP/S019367/1, EP/P02470X/1, and EP/P025285/1). [Correction added on 10 April 2024: Corrections in table 6 and data availability statement]\n\n### 10.2 Conflict of Interest\n\nThe authors declare no conflict of interest.\n\n### 10.3 Data Availability Statement\n\nThe data and code that support the findings of this study are openly available at [https://doi.org/10.15131/shef.data.25233514](https://doi.org/10.15131/shef.data.25233514), [https://doi.org/10.15131/shef.data.252507/0.v1](https://doi.org/10.15131/shef.data.252507/0.v1), and respectively.\n\n### 10.3 Keywords\n\nalloy design, high entropy alloys, high throughput computation, machine learning\n\n[1] J. W. Yeh, S. K. Chen, S. J. Lin, J. Y. Gan, T. S. Chin, T. T. Shun, C. H. Tsau, S. Y. Chang, _Adv. Eng. Mater._**2004**, \\(6\\), 299.\n* [2] B. Cantor, T. H. Chang, P. Knight, A. J. B. Vincent, _Mater. Sci. Eng._ A. Struct. Mater. **2004**, _375-377_, 213.\n* [3] E. P. George, D. Raabe, R. O. Ritchie, _Nat. Rev. Mater._**2019**, \\(4\\), 515.\n* [4] S. Altzami, P. Edalati, M. Fuji, K. Edalati, _Mater. Sci. Eng.: R. Rep._**2021**, _146_, 100644.\n* [5] F. He, Z. Wang, Q. Wu, S. Niu, J. Li, J. Wang, C. T. Liu, _Scr. Mater. **2017**, _131_, 42.\n* [6] D. B. Miracle, _Mater. Sci. Technol._**2015**, _31_, 1142.\n* [7] E. Pickering, N. Jones, _Int. Mater. Rev._**2016**, _61_, 183.\n* [8] D. B. Miracle, _JOM_**2017**, _69_, 231.\n* [9] D. B. Miracle, O. N. Senkov, _Acta Mater._**2017**, _122_, 448.\n* [10] K. Biswas, J.-W. Yeh, P. P. Bhattacharjee, J. T. M. DeHosson, _Scr. Mater._**2020**, _185_, 54.\n* [11] M. C. Troparencyk, J. R. Morris, P. R. C. Kent, A. R. Lupini, G. M. Stocks, _Phys. Rev. X_**2015**, \\(5\\), 011041.\n* [12] B. Gwalani, D. Choudhuri, K. Liu, J. T. Lloyd, R. S. Mishra, R. Banerjee, _Mater. Sci. Eng. A, Struct. Mater._**2020**, _771_, 138620.\n* [13] M. Laurent-Brocq, L. Perriere, R. Pires, Y. Champion, _Mater. Des._**2016**, _103_, 84.\n* [14] W. Ren, Y. F. Zhang, W. L. Wang, S. J. Ding, N. Li, _Mater. Des._**2016**, _235_, 112454.\n* [15] A. M. Manzoni, U. Clatzel, _Mater. Charact._**2019**, _144_, 512.\n* [16] J. Joseph, M. Sendezera, Q. Chao, K. F. Sharmalye, S. Rana, S. Gupta, S. Venkatesh, P. Hodgson, M. Bamett, D. Fabjanic, J. Alloys Compd. **2021**, _888_, 161496.\n* [17] Y.-T. Chen, Y. J. Chang, H. Murakami, S. Gorsse, A. C. Yeh, _Scr. Mater. **2020**, _187_, 177.\n* [18] A. Shafie, _Metail. Mater. Trans. A: Phys._**2022**, _53_, 4349.\n* [19] Y. Lu, Y. Dong, H. Jiang, Z. Wang, Z. Cao, S. Guo, T. Wang, T. Li, P. K. Liaw, _Scr. Mater._**2020**, _187_, 202.\n* [20] G. L. W. Hart, T. Mueller, C. Toher, S. Curtarolo, _Nat. Rev. Mater._**2021**, \\(6\\), 730.\n* [21] W. Huang, P. Martin, H. L. Zhuang, _Acta Mater._**2019**, _169_, 225.\n* [22] Y. Zeng, M. Man, K. Bai, Y.-W. Zhang, _Mater. Des._**2021**, _202_, 109532.\n* [23] Y. Yan, D. Lu, K. Wang, _Comput. Mater. Sci._**2021**, _193_, 110723.\n* [24] C. Molnar, _Interpretable Machine Learning A Guide for Making Black Box Models Explainable_**2022**.\n* [25] M. Cakyo-Dahlborg, S. Mehraban, N. P. Lavery, S. G. R. Brown, J. Combie, J. Cullen, J. Ciesla, Z. Leong, R. Goodall, U. Dahlborg, _J. Alloys Compd._**2021**, _865_, 158799.\n* [26] C. Wen, Y. Zhang, C. Wang, D. Xue, Y. Bai, S. Antonov, L. Dai, T. Lookman, Y. Su, _Acta Mater._**2019**, _170_, 109.\n* [27] A. Debnath, A. M. Krajewski, H. Sun, S. Lin, M. Ahn, W. Li, S. Priya, J. Singh, S. Shang, M. B. Seese, Z.-K. Liu, W. F. Reinhart, _J. Mater. Inf._**2021**, \\(1\\), 3.\n* [28] R. Machaka, _Comput. Mater. Sci._**2021**, _188_, 110244.\n* [29] D. Q. Zhao, S. P. Pan, Y. Zhang, P. K. Liaw, J. W. Qiao, _Appl. Phys. Lett._**2021**, _118_, 231904.\n* [30] Y. V. Krishna, M. K. jaiswal, M. R. Rahul, _Scr. Mater._**2021**, _197_, 113804.\n* [31] K. Kaufmann, K. S. Vecchio, _Acta Mater._**2020**, _198_, 178.\n* [32] U. Bhandari, C. Zhang, C. Zeng, S. Guo, A. Adhikari, S. Yang, _Crystals_**2021**, _11_, 1.\n* [33] C. Wen, C. Wang, Y. Zhang, S. Antonov, D. Xue, T. Lookman, Y. Su, _Acta Mater._**2021**, _212_, 116917.\n* [34] F. Liu, X. Xiao, L. Huang, L. Tan, Y. Liu, _Mater. Today Commun._**2022**, _30_, 103172.\n* [35] Y. Sun, Z. Lu, X. Liu, Q. Du, H. Xie, J. Lv, R. Song, Y. Wu, H. Wang, S. Jiang, Z. Lu, _Appl. Phys. Lett._**2021**, _119_, 201905.\n* [36] C. Zhang, Y. Yang, _MPS Stall._**2022**, _47_, 158.\n* [37] E. Prieto, A. Vaz-Bornero, J. Gonzalez-Julian, S. Guo, P. Alvaredo, _Int. J. Reflect. Mat. Mater._**2021**, _99_, 105592.\n* [38] E. P. George, W. A. Curtin, C. C. Tasan, _Acta Mater._**2020**, _188_, 435.\n* [39] X. Liu, J. Zhang, P. Pei, _Prog. Mater. Sci._**2023**, _131_, 101018.\n* [40] X. Liu, P. Xu, J. Zhao, W. Lu, M. Li, G. Wang,.-J. Alloys Compd._**2022**, _921_, 165984.\n* [41] R. Arroyave, _J. Phase Equ* [58] L. Breiman, _Mach. Learn._**2001**, _45_, 5.\n* [59] F. Pedregosa, C. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, _J. Mach. Learn. Res._**2011**, _12_, 2825.\n* [60] N. Islam, W. Huang, H. L. Zhuang, _Comput. Mater. Sci._**2018**, _150_, 230.\n* [61] H. Dalianis, _Clinical Test Mining: Secondary use of Electronic Patient Records_ (Ed: H. Dalianis), Springer International Publishing, Cham **2018**, pp. 45-53.\n* [62] B. Athi, A. Bajpai, N. P. Gurao, K. Biswas, _Modell. Simul. Mater. Sci. Eng._**2021**, _29_, 85005.\n* [63] M. J. Campbell, _Statistia at Square One_, Wiley Blackwell, Hoboken, NJ **2021**.\n* [64] C. Chen, L. Ma, Y. Zhang, P. K. Liaw, J. Ren, _Intermetallics_**2023**, _154_, 107819.\n* [65] S. Risal, W. Zhu, P. Guillen, L. Sun, _Comput. Mater. Sci._**2021**, _192_.\n* [66] L. Zhang, H. Chen, X. Tao, H. Cai, J. Liu, Y. Ouyang, Q. Peng, Y. Du, _Mater. Des._**2020**, _193_.\n* [67] J. J. Gilman, _Electronic Basis of The Strength of Materials_, Cambridge University Press **2003**.\n* [68] W. Hume-Rothery, _Elements of Structural Metallurgy_, Institute of Metals, London **1961**.\n* [69] W. D. Callister, D. G. Rethwisch, _Materials Science and Engineering_, Wiley, Hoboken, NJ **2015**.\n* [70] S. M. Lundberg, S.-I. Lee, in _Proc. of the 31st Int. Conf. on Neural Information Processing Systems_, Curran Associates Inc., Long Beach, CA **2017**, pp. 4768-4777.\n* [71] B. Cao, S. Yang, A. Sun, Z. Dong, T.-Y. Zhang,_J. Mater. Inf._**2022**, \\(2\\), 4.\n* [72] S. A. Kube, S. Sohn, D. Uhl, A. Datype, A. Mehta, J. Schroers, _Acta Mater._**2019**, _166_, 677.\n* [73] G. Pawley, _J. Appl. Crystallog._**1981**, _14_, 357.\n* [74] ASTM **1984**.\n* [75] Z. Wang, Y. Huang, Y. Yang, J. Wang, C. T. Liu, _Scr. Mater._**2015**, _94_, 28.\n* [76] F. Tian, L. K. Varga, N. Chen, J. Shen, L. Vtos, _Intermetallics_**2015**, _58_, 1.\n* [77] S. Fang, X. Xiao, L. Xia, W. Li, Y. Dong, _J. Non-Cryst. Solids_**2003**, _321_, 120.\n* [78] U. Mizutani, H. Sato, _Crystals_**2017**, \\(7\\), 9.\n* [79] A. Takeuchi, A. Inoue, _Mater. Trans._**2005**, _46_, 2817.\n* [80] S. Guo, C. T. Liu, _Prog. Nat. Sci.: Mater. Int._**2011**, _21_, 433.\n* [81] Y. Zhang, Y. J. Zhou, J. P. Lin, G. L. Chen, P. K. Liaw, _Adv. Eng. Mater_**2008**, _10_, 534.\n* [82] X. Yang, Y. Zhang, _Mater. Chem. Phys._**2012**, _132_, 233.\n* [83] J. M. Rickman, G. Balasubramanian, C. J. Marvel, H. M. Chan, M. T. Burton, _J. Appl. Phys._**2020**, _128_."
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nougat '/content/PDFs/MS&T - 2024 - Berry.pdf' --o 'Output'"
      ],
      "metadata": {
        "id": "j1tTNHorS5Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nougat '/content/PDFs/MS&T - 2024 - Berry.pdf' --o 'Output' --no-skipping --recompute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAoxbi7sO7x6",
        "outputId": "5cab6fce-8eba-4c24-e22b-4c8943a25d14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "/usr/local/lib/python3.11/dist-packages/nougat/transforms.py:146: UserWarning: Argument 'alpha_affine' is not valid and will be ignored.\n",
            "  alb.ElasticTransform(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "  0% 0/5 [00:00<?, ?it/s]INFO:root:Processing file /content/PDFs/MS&T - 2024 - Berry.pdf with 18 pages\n",
            "100% 5/5 [02:19<00:00, 27.97s/it]\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display.Latex('/content/Output/MS&T - 2024 - Berry.mmd')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ok4_H0hLR3uB",
        "outputId": "8e986aac-0a8b-444b-d40f-0c50bfadda28"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "# Supervised machine learning for multi-principal element alloy structural design\n\nJoshua Berry\n\nDepartment of Materials Science and Engineering, The University of Sheffield, Sheffield, UK\n\nKaterina A. Christofidou\n\nDepartment of Materials Science and Engineering, The University of Sheffield, Sheffield, UK\n\nSage\n\n###### Abstract\n\nThe application of supervised Machine Learning (ML) in material science, especially towards the design of structural Multi-Principal Element Alloys (MPEAs) has rapidly accelerated over the past five years. However, several factors are limiting the impact that these ML methodologies can have, chief amongst them being the availability and fidelity of data. This review analyses how ML has been utilised to accelerate the design of novel structural MPEAs, outlining the standard procedures followed, and highlighting the successes and common pitfalls identified in current studies. The need for experimental validation and incorporation into closed loop ML pipelines is also discussed, including the influence and integration of manufacturing methodologies into the ML decision making process.\n\nmulti-principal element alloys, higher tryp alloys, complex concentrated alloys, compositionally complex alloys, machine learning, experimental data, alloy design\n\n## Introduction\n\nHigh Entropy Alloys (HEAs), first introduced to the scientific community in 2004 by Yeh et al. [1] and Cantor et al. [2] respectively, are conventionally defined as a class of alloys containing five or more elements in either equiatomic elemental concentrations, or elemental concentrations in the range of 5 to 35 at.% [1]. This concept leads to HEAs occupying a vast uncharted compositional space [3] and sparking a wealth of studies and debates in the literature, not least on appropriate naming conventions. Consequently, several different terms have been proposed and are used to encompass different classes of materials such as, multi-component alloys, compositionally complex alloys, complex concentrated alloys or indeed the broader term, Multi-Principal Element Alloys (MPEAs). Concurrently, the term HEA has evolved to more routinely describe single phase MPEAs [4, 5]. For consistency in this review, MPEA will be used to refer to all subclasses listed above. Recent review articles and critical assessments of the MPEA field are available for readers unfamiliar with the background and application of these materials [4, 5, 6, 7, 8].\n\nMachine Learning (ML) is a methodology whereby computer systems can learn to perform specific data-based tasks without any explicit programming. Broadly speaking, ML can be split into three different categories, supervised, unsupervised and reinforcement learning [9]. This review will focus on the application of supervised learning, where ML models are fit on data containing known target outputs. Hence, the model can be trained to recognise patterns and trends in the available input data to predict the output [10]. Supervised learning can be further subdivided into classification and regression tasks. Classification is used to categorise the discrete values of a variable and separate the predictions into different categories. In contrast, regression is used to predict continuous numerical values [9].\n\nWith microstructural simplicity being a founding principle of the field, efforts have been made to develop rules to enable the prediction of the formation of solid-solution phases. These rules commonly take the form of two-dimensional phase stability plots and are based on the Hume-Rothery, Gibbs free energy and valence electron criteria [11, 12, 13, 14, 15, 16, 17, 18]. Furthermore, a significant number of experimental studies have been conducted to determine the microstructural and mechanical properties of different MPEA compositions, fabricated through a variety of manufacturing techniques as well as evaluating the impact of post processing methods [19, 20, 21, 22, 23, 24]. The generation of rules describing the phase stability of MPEAs, continuous experimental data collection, and vast compositional space, results in a\n\n###### Abstract\n\nThe data analysis of the data data is performed on a set of 1252 data points, which are used to determine the number of data points in the data set. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, respectively. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, respectively. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, respectively. The data set is divided into two subsets, each containing a set of 1252 data points, and each containing a set of 1252 data points, and each containing a set of 1252 data points, respectively.\n\nKaufmann et al.[43] supplemented 134 experimental compositions with 1664 DFT compositions. This use of CALPHAD and DFT to generate data for ML training raises the question, if it is possible to generate data in this way to train ML models, why use ML? There are multiple reasons for this. Firstly, CALPHAD is calculated based on energy minimisation of experimental data, which for most alloys, often requires extrapolation from binary and ternary systems that comprise the thermodynamic databases. Secondly, ML is significantly faster and incurs lower computational costs[38] than both CALPHAD and DFT calculations. Vazquez et al.[35] reported that their neural network is 436 times faster than the CALPHAD method, and Kaufmann et al.[28] stated that DFT can take 100s of hours of computation per composition.\n\nAs illustrated in Table 1, MPEA data on properties such as phase, collected under a variety of experimental conditions, are published in small datasets across a range of literature. Therefore, it would seem logical to aggregate these data sources to enhance the quantity of MPEA data for ML model training. However, Ottomano et al.[62] argue that expanding datasets in this way may affect the organicity and overall quality of the available data. To demonstrate this, model performance of a range of algorithms was compared before and after aggregation using a variety of collation techniques. In all cases, classical ML algorithms such as random forest and logistic regression demonstrated a reduction in performance. In contrast, deep learning models showed greater robustness, but no significant change in performance. Therefore, when collating data to increase data quantity, it is critical to consider the type of ML algorithm being utilised and the difference in origin of the individual data.\n\nOther methods to combat insufficient data can be as simple as employing an empirical relationship. For example, Equation 1 was used by Huang et al.[27] to convert reported yield strength data \\(\\sigma_{y}\\) to the target variable hardness \\(HV\\). However, such practices come at the risk of introducing low quality data into the training dataset. Equation 1 for example, has been found to be accurate for BCC MPEAs, but less so for FCC MPEAs[27]. Thus, such practices are generally not recommended for MPEA studies, unless sufficient evidence is present to ensure the fidelity of generated data. Alternatively to physics based or empirical modelling approaches towards data generation, ML can also be used to generate data for ancillary ML algorithms. Lee et al.[63] produced a Conditional Generative Adversarial Network (CGAN) to generate additional training data, and demonstrated that employing a CGAN in conjunction with a neural network can improve phase prediction performance for MPEAs. However, the number and diversity of the generated samples depends on the original dataset. Hence, on smaller datasets, CGAN augmentation has limited impact on model performance[32]. Finally, Pilania et al.[10] discusses the potential shift in focus towards utilising natural language processing as a technique to strip materials knowledge and data from published literature. This could efficiently compile existing materials knowledge to produce substantial databases for MPEA based ML studies[64, 65, 6].\n\n\\[\\sigma_{y}(MPa)\\approx\\frac{1}{3}H(MPa)=\\frac{9.81}{3}HV(Hv) \\tag{1}\\]\n\nTackling the imbalance of MPEA data, highlighted in Figures 2 and 3, is an entirely different problem. Traditional classification tasks struggle with imbalanced data as they have a tendency to categorise data into the majority class[31, 39], which also tends not to be the class of interest in the majority of studies. For small imbalanced datasets the minority class is typically insufficient for learning, especially when there is a degree of overlap within the classes[31]. This is especially true for MPEA databases, where FCC and BCC phases dominate, and pronounced overlap exists between solid-solution and solid-solution plus intermetallic phases[6].\n\nThe most common methods to tackle the issue of imbalanced data are random oversampling and undersampling[31]. Oversampling supplements the available training data with\n\nFigure 1: Example of a typical ML study workflow or pipeline within materials science. Reprinted with permission from[30]. ©2024 American Chemical Society\n\nclones of the minority classes in order to balance them with the majority class.[66] Risal et al.[66] applied random oversampling and found it to be effective for phase prediction based on the available dataset. Ren et al.[33] also utilised random oversampling to change the distribution of their datasets and although the authors did not directly comment on the impact of the data augmentation, they successfully produced two models to predict high hardness MPEAs. In contrast, undersampling modifies the class distribution by reducing the data of every class to match the minority class.[66] This technique is significantly less popular in the MPEA field as it means the removal of precious data from already small datasets. Hence, oversampling is generally preferred. Another technique to combat data imbalance is Synthetic Minority Oversampling Technique (SMOTE)[67]. SMOTE generates synthetic samples based upon the minority classes of the input dataset, using k-nearest neighbours to sample the feature space of the class, such that the distribution of data across all the classes is balanced. Crucially, this method increases the number of datapoints while maintaining the original trend[39, 68]. Bansal et al.[68] reported an improvement in model prediction by increasing the quantity of datapoints using synthetic data. However, Singh et al.[69] argue that implementing augmented data to combat data imbalance is not reliable. The authors claim that accuracy alone is not the most robust measure for assessing performance from ML models constructed from imbalanced data and, that it cannot be guaranteed the generated samples are MPEAs. To investigate this, multiple \"vanilla\" ML classifier models were compared with SMOTE-Tomek augmented models of the same algorithms. The best performing classifier was found to be a random forest model and despite the claims that augmenting data is not the optimal approach the augmented models showed better performance scores in all cases than the vanilla models. Hareharen et al.[70] also find that SMOTE improves the ML model's ability to differentiate between the various classes.\n\n## Multi-principal element alloy feature selection\n\nSupervised ML tasks aim to construct models to predict a target variable from a set of input features.[9] In the case of MPEAs, the target is often phase formation[71, 47] as a proxy for microstructure, which dictates structural and mechanical properties. Thus, the input features are most commonly empirical relations based on the atomic properties of the alloys' constituent elements describing electronic, thermodynamic, physical and chemical characteristics.[32, 72] Table 2 summarises several examples of features frequently utilised in MPEA ML models.\n\n\\begin{table}\n\\begin{tabular}{l l l l} \\hline \\hline Authors & Year & Material Properties included & Processing Method \\& History included & Number of Datapoints \\\\ \\hline Ye et al.[50] & 2016 & Phase & All As-Cast & 118 \\\\ Miracle et al.[6] & 2017 & Phase as SS or IM. Material properties are mentioned & Yes (Processing and post-processing) & 648 \\\\  & but not explicitly included. & & \\\\ Gorsese et al.[51, 52] & 2018 & Phase, \\(\\rho\\), HV, \\(\\sigma^{I}\\), \\(\\sigma^{II\\pi}\\), \\(\\varepsilon\\), E & N/A & 370 \\\\ Couzinie et al.[53] & 2018 & Phase, \\(\\rho\\), \\(\\sigma^{I}\\), E & Yes (Processing) & 340 \\\\ Roy et al.[54] & 2020 & Phase, (\\(\\bar{E}\\)) & N/A & 340, (107) \\\\ Borg et al.[55] & 2020 & Phase, \\(\\rho\\), HV, \\(\\sigma^{I}\\), \\(\\sigma^{I\\pi}\\), \\(\\varepsilon\\), E & Yes (Processing) & 1545 \\\\ Machaka et al.[56] & 2021 & Phase & Yes (Processing and post-processing) & 1360 \\\\ Detor et al.[57] & 2022 & HV & Yes (Processing) & 86 \\\\ Han et al.[34] & 2022 & Phase & Yes (Processing, all as-cast) & 1138 \\\\ Chen et al.[58] & 2022 & Phase, Strength, Low-Cycle Fatigue, High-Cycle Fatigue and Fatigue Crack Growth Rate & Extensive processing methodology and history & 66 \\\\ \\hline \\hline \\end{tabular} \\(\\rho\\) denotes the density, HV represents the Vickers hardness, \\(\\sigma^{I}\\) is the yield strength, \\(\\sigma^{II\\pi}\\) is the ultimate tensile strength, \\(\\varepsilon\\) is the strain elongation and \\(\\bar{E}\\) represents the Youngs modulus.\n\n\\({}^{\\star}\\)Corrigendum\n\n\\end{table}\nTable 1: Experimentally calculated MPEA databases published in peer-reviewed literature.\n\nFigure 2: Data imbalance and domination of FCC and BCC phases across 648 published reports of microstructure within MPEA studies. Phases appearing less than 4 times are not shown.[6] ©2024 reused with permission from Elsevier\n\nFeatures, such as those summarised in Table 2, often originate from the Hume-Rothery rules on solid-solution formation for binary systems. For example, the need for small atomic size differences, comparable valency and similar electronegativities [80]. These features have been shown to display clear trends with MPEA microstructural and mechanical properties, thus, successfully translating to MPEA ML studies. Guo et al. [13] determined Valence Electron Concentration (VEC) to be the physical parameter controlling formation of FCC (VEC \\(\\geq\\) 8) or BCC (VEC \\(<\\) 6.87) solid solutions. This observed phase formation has a strong impact on mechanical properties with BCC phases typically observed to display a higher hardness [81], but a lower ductility compared to their FCC counterparts [82, 83, 8]. Similarly, Wang et al. [18] developed a new parameter, \\(\\gamma\\) to describe atomic packing as an improvement to the commonly accepted atomic size difference, \\(\\delta\\), where \\(\\gamma\\) \\(<\\) 1.175 results in solid-solution formation. Indeed, these trends have also been observed when applied to ML studies, with VEC and \\(\\delta\\) being reported as two of the most important features by many authors investigating both microstructural and mechanical properties [27, 33, 41, 42, 84], demonstrated in\n\n\\begin{table}\n\\begin{tabular}{l l l l} \\hline Feature & Name & Equation & Reference \\\\ \\hline \\(\\delta\\) & Atomic Size Difference & & 18,7,3,74 \\\\ \\(\\gamma\\) & Atomic Packing Parameter & \\(\\gamma=\\frac{\\mathrm{m}_{\\mathrm{m}}}{\\mathrm{m}_{\\mathrm{i}}}\\) & 18 \\\\  & & \\(\\omega_{\\mathrm{x}}=1-\\sqrt{\\frac{\\mathrm{m}_{\\mathrm{i}}+\\mathrm{i}^{2}- \\mathrm{i}^{2}}{\\mathrm{m}_{\\mathrm{i}}+\\mathrm{i}^{2}}}\\) & \\\\ \\(\\Delta\\chi\\) & Electronegativity Difference & \\(\\Delta\\chi=\\sqrt{\\sum\\limits_{i=1}^{n}\\mathrm{c}(\\zeta_{i}-\\bar{\\chi})^{2}}\\) & 75 \\\\ VEC & Valence Electron Concentration & VEC \\(=\\sum\\limits_{i=1}^{n}\\mathrm{c}(\\sqrt{\\mathrm{EC}}\\mathrm{\\zeta})_{i}\\) & 13,76 \\\\ e/\\(a\\) & Number of Itinerant Electrons per Atom & \\(\\frac{a}{a}=\\sum\\limits_{i=1}^{n}\\mathrm{c}(\\frac{\\delta}{a})\\) & 13,76,77 \\\\ \\(\\Delta H_{\\mathrm{mix}}\\) & Enthalpy of Mixing & \\(\\Delta H_{\\mathrm{mix}}=4\\sum\\limits_{i=1}^{n}\\mathrm{c}\\zeta_{i}\\Omega_{i}\\) & 11,14,78 \\\\ \\(\\Delta S_{\\mathrm{mix}}\\) & Entropy of Mixing & \\(\\Delta S_{\\mathrm{mix}}=-R\\sum\\limits_{i=1}^{n}\\mathrm{c}\\ln\\mathrm{c}_{i}\\) & 8,14 \\\\ \\(T_{m}\\) & Weighted Melting Temperature & \\(T_{m}=\\sum\\limits_{i=1}^{n}\\mathrm{c}(T_{m})_{i}\\) & \\\\ \\(\\Omega\\) & Yang Parameter & \\(\\Omega=\\frac{T_{m}\\sum_{i=1}^{n}\\mathrm{c}\\left(1-\\frac{\\mathrm{c}}{\\mathrm{c }}\\right)^{2}}\\) & 79 \\\\ \\hline \\end{tabular} Quantities denoted with a bar indicate that it is the average value, while quantities with an \\(i\\) indicate the \\(i\\)/th element. \\(c\\) represents the atomic fraction of the element, \\(r\\) denotes the atomic radii and \\(\\bar{r}\\) represents the Youngs modulus. \\(R\\) is the molar gas constant. For \\(\\gamma\\), \\(\\alpha_{i}\\) denotes the solid angles around the largest and smallest atoms, represented by subscript \\(\\mathrm{L}\\) and \\(\\bar{r}\\) respectively. In \\(\\Delta H_{\\mathrm{mix}}\\), \\(\\Omega_{\\mathrm{\\bar{\\mathrm{\\mathrm{\\mathrm{\\mathrm{\\mathrm{\\mathrm{\\mathrm{ \\mathrm{\\mathrm{\\mathrm{\\mathrm{\\mathrm{\\mathrm{\\mathrm{\\mathrm{\\mathrm{\\mathrm{\\mathrm{ \\mathrm{       }}}}}}}}}}}}}}}}\\) is the enthalpy coefficient for elements \\(i\\) and \\(j\\) respectively.\n\n\\end{table}\nTable 2: Examples of features commonly used in ML models in the MPEA field\n\nFigure 3: Classification of solid solution and intermetallic phases within MPEA data. (a) Microstructure classification by phase type with sub-classification by number of phases, (b) Number of phases classification with sub-classification by type of phase.\\({}^{\\mathrm{d}}\\) ©2024 reused with permission from Elsevier\n\nFigure 4. This importance is defined and measured according to how much impact the feature has on the models decision making process.\n\nAn important decision when selecting features for MPEA ML studies is whether to include the chemical composition of the alloy as an input feature. Bakr et al. [45] demonstrated that it is possible to train an artificial neural network for phase and hardness prediction of MPEAs using only the chemical composition as the input feature. Similarly, Jain et al. [46] included the elemental compositions as features for phase predictions and used them as the only feature for hardness predictions. However, Wen et al. [86] argue that utilising the elemental composition, in conjunction with the elemental property features discussed above, significantly out-performs just training ML models on the elemental composition. Furthermore, Morgan et al. [25] state that utilising composition as an input feature means that the model cannot be used to extrapolate to systems including elements outside of the training data. Hence, they argue that it is better to represent each element with elemental properties to enable feature generation by taking compositionally averaged combinations of the constituent elements, as in Table 2. If utilising ML to optimise the composition of an already defined MPEA system, then including composition may be useful. For example, Chen et al. [41] include the molar fraction of the six constituent elements of their MPEA system as features for the model, but only hardness optimisation of a single alloy system was investigated. Alternatively, if the goal of ML application is materials discovery, then evidence suggests prioritising elemental property features.\n\nSubsequent to data collection, feature engineering is the next step in model construction and consists of feature generation and selection. Firstly, the features chosen for any ML model must be both machine readable and relevant to the target variable [25]. Following the discussion of the need for high quantities of data to train ML models, it could be assumed that the more features available to make predictions of the target, the better a model will perform. However, this is not the case and many MPEA ML studies demonstrate that as the number of features increases, prediction accuracy plateaus [33, 36, 38, 36, 37, 38] as illustrated in Figure 5. Thisalso introduces feature redundancy, overfitting and results in poor generalisability and computational efficiency [33, 38, 25]. Feature selection is an essential process that reduces the dimensionality of the ML task by identifying and removing irrelevant, noisy and redundant features Simultaneously feature selection retains sufficient information and enables optimisation of the number and combination of features to maximise accuracy and improve ease of training [31, 32, 33, 34, 38, 47, 89].\n\nCollinearity of features is detrimental to model performance [33], computational efficiency and, crucially, interpretability, Further, it restricts the ability to ascertain the individual contribution of each feature to the model [90]. Hence, for features that are correlated, the least important feature is typically omitted [40, 44]. To detect collinearity, the most popular method is to measure the correlation of individual features using the Pearson Correlation Coefficient (PCC) [34, 91], given by Equation 2.\n\n\\[r=\\frac{\\sum_{i=1}^{n}{(x_{i}-\\bar{x})(y_{i}-\\bar{y})}}{\\sqrt{\\sum_{i=1}^{n}{ (x_{i}-\\bar{x})^{2}}}\\sqrt{\\sum_{i=1}^{n}{(y_{i}-\\bar{y})^{2}}}} \\tag{2}\\]\n\nWhere \\(x\\) and \\(y\\) denote two of the features and \\(\\bar{x}\\) and \\(\\bar{y}\\) represent the mean of the two features, respectively. PCC values can range from \\(+1\\) to \\(-1\\), with positive values indicating a positive relationship between the variables and vice versa. Commonly in correlation analysis, features with PCC values\\(>0.80\\) are considered very strongly correlated [33, 41, 91]. However, there are no fixed cut-offs for the interpretation of feature correlation strength, rather context is critical to understanding both the extent and impact of the correlation on the model [92, 93]. Despite this, in many MPEA ML studies, the limit for where features are considered highly correlated\n\nFigure 4: Feature importance scores of 15 features within an XGBoost model used for phase classification. In this case R denotes atomic radius and VE represents valence electron count [85] ©2024 adapted with permission from Elsevier\n\nFigure 5: Model performance plateaus as the number of features is increased, in this case for a support vector machine model. The red and blue lines represent the average and lowest prediction error for that number of features respectively [88]. ©2024 reused with permission from Elsevier\n\nis often set very close to 1, at PCC \\(>\\) 0.95[40, 84, 88]. Typically, in MPEA ML studies where the correlations are moderate, the issue of where to place the limit of correlation is avoided and authors often do not state at what point they would have considered features to be correlated and why[42, 44, 72]. It is also common for MPEA ML studies to assess how important ML features are through their correlation with the outputs[46], but this is not an effective method for model interpretability.\n\nData, features, and models are extensively interlinked and so the optimal feature combinations for the available data must be found for each model tested. However, it can be computationally prohibitive to test every possible permutation[25]. Several techniques and approaches have been developed to enable feature engineering. The first and simplest method of feature selection is human context. Nonsensical features and those with no relation to the target variable should be removed[25]. Sequential feature selection iteratively adds or subtracts features from the dataset in order to maximise model performance[25, 33, 87]. Li et al.[88] utilised a genetic algorithm to find the optimal combination of features. The genetic algorithm mimics the mechanism of natural selection to arrive at the global optimum performance without testing every possible combination. This saves on computational efficiency over the exhaustive sequential feature selection methods, especially for large feature spaces. MPEA ML studies mostly agree that fewer features (4 or 5) are optimal for predicting phase formation[38, 86, 87, 88] and mechanical properties[29]. However, although Huang et al.[40] agrees that the optimal number of features for phase selection is small (5), they find that the optimal number of features for hardness prediction is much larger (13), in contrast to the majority of MPEA studies.\n\n## Machine learning algorithms for multi-principal element alloy design\n\nAn exhaustive discussion on how each individual supervised ML algorithm functions is beyond the scope of this review. See Hastie et al.[71] and James et al.[90] for an overview. No supervised ML algorithm is by default superior to any other and the choice of optimal algorithm depends strongly on the available data and target output[87, 30]. Choosing a suitable algorithm is critical for improving model performance and efficiency[33]. Therefore, many ML studies trial a handful of potential ML algorithm candidates to determine which performs the best, for example[34, 40, 87], shown in Figure 6. Alternatively, some ML MPEA studies already have a specific algorithm in mind because of the benefits of that particular methodology. For example, Bhandari et al.[44] and Choudhury et al.[94] select a random forest algorithm for its simplicity and interpretability.\n\nSimple linear regression models are much easier to interpret but lack in predictive power and performance[30]. Clustering algorithms, such as k-nearest neighbours, often suffer from low accuracy and when the data is imbalanced, the more frequent classes significantly dominate predictions, becoming more noticeable as the number of neighbours considered increases[27, 34]. Tree based algorithms have very high interpretability and training speed but are susceptible to overfitting. However, ensemble tree based models such as random forest have anti-overfitting properties[34, 10]. In contrast, support vector machines and neural networks can provide a boost in predictive performance at the cost of model interpretability and the need to scale input data. Lastly, neural networks require significantly more computational power and training time for small gains in performance[30, 31, 10]. Figure 7 provides an illustration of this transparency to performance trade-off.\n\nTo assess the predictive performance of supervised ML models a range of metrics are available, split between the classification and regression tasks, that assess the difference between the model's predicted outputs and the true outputs from the validation data set[25]. For classification tasks, accuracy, precision, recall, and F1-score are the most common performance metrics in MPEA ML studies, provided in Equation 3[94, 45, 47, 99]. A complete description of these can be found in Dalianis[95].\n\n\\[Accuracy=\\frac{\\mathit{TP}+\\mathit{TN}}{\\mathit{TP}+\\mathit{TN}+\\mathit{FP}+ \\mathit{FN}}\\] ( 3 \\[a\\] )\n\n\\[Precision=\\frac{\\mathit{TP}}{\\mathit{TP}+\\mathit{FP}}\\] ( 3 \\[b\\] )\n\n\\[Recall=\\frac{\\mathit{TP}}{\\mathit{TP}+\\mathit{FN}}\\] ( 3 \\[c\\] )\n\n\\[F_{1}\\;Score=2\\times\\frac{\\mathit{Precision}\\times\\mathit{Recall}}{\\mathit{ Precision}+\\mathit{Recall}}\\] ( 3 \\[d\\] )\n\nTrue positives (TP) are the data points that are correctly predicted by the model for a class. False negatives (FN) are data points that are incorrectly predicted as a different class by the model. False positives (FP) are data points\n\nFigure 6: Comparison of model performance across the same experimental database. Each column represents a different feature subset selected by application of the models indicated in the legend[97]. ©2024 adapted with permission from Elsevier\n\nthat are a different class but are predicted to be the class under consideration. True negatives (TN) are data points that are a different class to the one under consideration and are predicted to be a different class, thus the class being considered is not involved. For regression models, Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) are the two most common metrics that MPEA ML models will aim to minimise, Equation 4a and 4b respectively [38, 39, 33, 25, 32, 33]. In addition, the coefficient of determination (R2), Equation 4c, combined with parity plots of predicted vs actual data, provides an informative assessment of model quality, with values > 0.7 typically indicating a useful model [25].\n\n\\[RMSE=\\sqrt{\\frac{\\sum_{i=1}^{n}{(y_{i}-\\hat{y}_{i})^{2}}}{n}} \\tag{4a}\\]\n\n\\[MAE=\\frac{\\sum_{i=1}^{n}{|y_{i}-\\hat{y}_{i}|}}{n} \\tag{4b}\\]\n\n\\[R^{2}=1-\\frac{\\sum_{i}{(y_{i}-\\hat{y}_{i})^{2}}}{\\sum_{i}{(y_{i}-\\bar{y})^{2}}} \\tag{4c}\\]\n\nWhere \\(y\\) denotes the true value, \\(\\hat{y}\\) represents the predicted value, \\(\\bar{y}\\) represents the mean of the true data, and \\(n\\) is the number of data points. To enable the model performance to be correctly validated using the metrics described above, the available data first needs to be split into training, validation and testing in a reproducible manner [30]. The training data is used by the model to relate the input features to the desired output. The validation data is then used to assess the performance of the model on the training data and optimise the chosen algorithm. Subsequently, the testing data is used to assess the error in the final optimised version of the model [30, 25].\n\nThere are two common ways to split the data to train an ML model and assess its performance. The first is Train-Test-Split where the database is simply split into fixed fractions (e.g., 80% - 10% - 10%). However, this can cause poor generalisability in the data across the fractions if the data distribution is imbalanced [33]. Bakr et al. [45] reported a significant difference between the RMSE of the training and testing data due to a different distribution of data across the two sets. The second approach is k-fold cross-validation, Figure 8. In this method the dataset is divided into \\(k\\) equal fractions, with each fraction used once as validation data while the other \\(k\\)-1 fractions are used for training. After \\(k\\) iterations, the model will have been trained and validated \\(k\\) times on the same database, eliminating the effect of sampling. This enables the average performance metrics to be calculated, providing a more appropriate assessment of model performance, and is particularly useful for small datasets, such as those in the MPEA field [33, 34, 27]. However, while the majority of MPEA ML studies detail the methodology they have used to assess the performance of the model on the available data, very few discuss retraining the final model using the whole database. This is a key step to maximise the amount of training data. As it is not often discussed in the published literature, it is unclear if this step is performed in MPEA ML studies, with the notable exception of Liu et al. [36].\n\nHyperparameter optimisation is a key step in the development of ML models and features in the vast majority of MPEA ML studies. Hyperparameters are guidelines for the ML model construction that control the learning process [30]. For example, they could be controlling the maximum number of trees and branches in a random forest or the number of nodes and layers in a neural network [72]. It is important to optimise the hyperparameters of an ML model to maximise the model performance and speed [33, 96]. The two most common methods of hyperparameter optimisation within MPEA ML studies are grid search and Bayesian optimisation. For grid search, a range for each hyperparameter is specified and each possible combination of these parameters is tested against a baseline to find the optimal combination. This can also be performed randomly rather than exhaustively to save on computation time. In contrast, Bayesian optimisation minimises the loss on an error function to find the optimal hyperparameters for a model. Neither method is by definition better than the other, but both have been used successfully in MPEA ML studies [39, 84]. Hyperparameter optimisation is also coupled with cross-validation to provide a more accurate assessment of the impact of hyperparameter changes on performance [33, 43].\n\nTable 3 summarises many recent studies where ML has been utilised to predict MPEA microstructural and mechanical properties, demonstrating the rapid adoption of ML in the field, in contrast to the lack of development in data availability, Table 2. As illustrated in Table 3 it is clear that the majority of ML models target alloy phase formation, because of the strong dependence of mechanical properties on microstructure. Of the alloy mechanical properties, hardness is the most commonly investigated in ML based MPEA studies, likely due to the ease of experimental measurement to validate model predictions and generate data [3].\n\nPhase formation prediction is a complex classification task with many possible phases that can form within\n\nFigure 7: Trade-off between model transparency and model predictive performance [10]. ©2024 reused with permission from Elsevier\n\nMPEAs. Hence, many MPEA ML studies drastically condense the phase space in order to simplify the ML task and improve performance. For example, the MPEA database produced by Machaka et al.[56] contains 35 distinct labels for the phases of different compositions, which is simplified to 7 distinct labels. Furthermore, many MPEA ML studies reduce the classification problems down to binary and tertiary cases where the model is predicting whether or not solid solution formation occurs, or some form of solid solution, intermetallic or amorphous phase respectively.[27, 34, 27, 94] This reduction in the number of classes is helpful to obtain meaningful results from the ML, but can be exaggerated in the aim of maximising performance.\n\nThe studies in Table 3 mostly utilise standard ML algorithms with a small number of input features based on the intrinsic properties of the constituent elements. Furthermore, these studies typically utilise small databases, often with insufficient data to effectively train the more complex models, such as neural networks. In some cases these studies are lacking experimental validation of the model outputs, availability of data to enable future iterations, and sufficient consideration of the impact of manufacturing on MPEA design.\n\n## From machine learning to multi-principal element alloy design\n\nOnce the models have been successfully constructed, trained and their performance assessed, the next step is to use the model to predict on unseen data. To do this most MPEA studies generate a virtual candidate search space. This involves selecting the desired elements and considering every possible combination in equimolar ratios, in a system of chosen size. For instance, Zhang et al.[87] considered 30 different elements in MPEA systems containing 4-6 elements in equimolar ratios. This produced a total of 763,686 unexplored alloy compositions for the ML to make predictions on the phase formation. This search space can be expanded to cover non-equimolar compositions, with each element typically being varied between 5 and 35 at.% with a compositional granularity of 1 at.%.[84, 86] In some MPEA studies, constraints are placed on the search space to further narrow it down. For example, in the study by Akhil et al.[39], the search space was reduced by first applying the constraints, \\(\\delta\\)\\(<\\)6.6% and \\(-\\)15\\(<\\)_Hmix_\\(<\\)5 kJ/mol to aid in the search for HCP forming MPEAs. However, this is an unnecessary step as ML is a very fast process and if this constraint was suitable for mapping HCP formation, then it would be expected to be predicted accordingly by the ML model if trained correctly.\n\nAlternatively, an inverse design methodology can be utilised.[104] In this case the final output property criteria is first specified as an input. The trained ML model then predicts feature configurations and hence, alloy compositions, that could satisfy the chosen property criteria.[105, 101] Yang et al.[84] utilised an inverse design methodology by projecting samples in the optimal property zone of the performance space and then deducing the features yielding this performance by inverse projection. Finally, the MPEA composition closest to the performance projection point by Euclidean distance was selected as the alloy composition to yield the high-performance property and was experimentally fabricated. From this inverse projection three MPEA compositions were downselected, synthesised and\n\nFigure 8: Comparison of the train-test-split and cross-validation model performance assessment methodologies. Highlights how cross-validation allows the model to be trained and validated multiple times before final testing.\n\ncharacterised. The best performing of these compositions exhibited a hardness 24.8% higher than the highest composition in the original dataset. Guo et al. [106] also apply inverse projection, but to optimise the composition of a preselected MPEA system, AlCrFeNiTi, to maximise hardness. From this inverse projection, four of the optimised composition alloys were experimentally synthesised. The highest hardness of these demonstrated a hardness 21.5% higher than any composition in the original dataset.\n\nDebnath et al. [107] performed a comparative study of forward and inverse design methodologies with a case study on the ultimate tensile strength of refractory MPEAs. The authors observed that the inverse design methodology could identify a composition with the same constraints as the forward scheme, but produced a higher target property prediction. Additionally, the authors state that for the inverse design case the addition of new conditions and generation of new candidates is incredibly fast compared to the forward case, which needs to be regenerated with each iteration. However, inverse design is not without its disadvantages. Often numerous feasible solutions are presented by the inverse ML due to a greater number of variables than constraints and, minor variations in the desired property can produce significant changes in alloy composition prediction [108].\n\n## Experimental validation\n\nAs previously discussed, the success and development of ML within the MPEA field is intrinsically linked to the experimental exploration of the compositional space. However, a significant number of MPEA ML studies do not contain any form of experimental validation [39, 44, 28, 45]. The most common approach to experimental validation is to fabricate a small number of downselected samples, typically between 5 and 10, via vacuum arc or induction melting, or through powder metallurgy routes [109].\n\nOnce the samples have been fabricated, the standard approach is to investigate the microstructure and mechanical properties to validate the results of the ML predictions and discover new compositions with exceptional properties. The vast majority of MPEA ML studies perform this investigation via the application of scanning electron microscopy, in conjunction with energy dispersive x-ray spectroscopy, to determine the microstructure, compare the nominal to actual composition, and observe any elemental segregation. X-ray diffraction is also used to further crystallographically analyse the phase formation within the alloy. Mechanical property assessments, most commonly hardness, are performed using Vickers microhardness indentation [33, 38, 40, 86, 87, 96]. Examples of this experimental assessment are provided in Figure 9. Additional analysis techniques such as electron backscattered diffraction [34], transmission electron microscopy and selected area diffraction [99] can enable greater insight into the crystallographic phase formation of the fabricated MPEAs, but this is typically above the resolution needed for ML model validation. However, there is a clear absence of extensive mechanical testing and post processing of the alloys, with Wang et al. [101] being one of the few to measure tensile properties of MPEAs following different post processing regimes predicted through ML.\n\nThe manufacturing methodology used to synthesise MPEAs significantly impacts alloy microstructure and phase formation, which governs the materials mechanical properties and overall performance [36, 45, 110, 111]. Post processing heat treatments can also further alter alloy microstructure and improve alloy mechanical performance [112]. For example, Otto et al. [113] demonstrated that even the most exemplar single-phase solid solution MPEA system, CrMnFeCoNi, is not always single phase. After annealing at intermediate temperatures (\\(<\\)700\\({}^{\\circ}\\)C), it will decay to form several secondary precipitates. Hence, incorporating the manufacturing and processing history into the ML task would assist the models to make accurate predictions irrespective of synthesis methodology.\n\nFor small scale trials of new MPEA compositions, arc melting is the dominant method for MPEA manufacture [114], especially for refractory MPEAs due to the high arc temperature [110]. However, the formation of defects such as cold shuts [115], cracking and elemental segregation are common in arc-melted ingots [110, 114]. Thus, the microstructure and mechanical properties can vary significantly throughout the ingot in the as-cast state [114]. Additive manufacturing processes offer faster cooling rates than conventional casting, resulting in a much finer grain structure, potentially enhancing mechanical properties [112]. However, the microstructures are more complex due to the different temperature histories and heating of subsequent layers [112]. In both cases, subsequent heat treatments can initiate phase transformations and improve mechanical properties. Homogenisation of MPEAs typically leads to higher strength than their as-cast counterparts [111] as well as mitigating or eliminating defects [112].\n\nDespite this, the majority of MPEA literature presents microstructure and mechanical properties in the as-cast condition [69, 114], which is not indicative of industrial applications. Furthermore, the majority of MPEA ML studies don't appropriately account for processing conditions [45, 116], select only as-cast data or omit any samples not in the as-cast state and manufactured via arc melting [33, 40, 84, 86, 87, 88]. The first barrier to the implementation of processing to MPEA ML studies is the lack of information available in publications, as it is not consistently reported within the literature [40, 45]. In addition, Machak [89] reported that post processing heat-treatment features performed poorly in classification of phase formation tasks.\n\nTo incorporate the effect of manufacturing into the ML process Zhu et al. [32] inserted the fabrication method into the input layer of a deep neural network for prediction of hardness by one-hot encoding. This is a process where categorical variables are mapped to integers so that they can be interpreted by the ML model. However, heat treated and laser remelted values of hardness were removed from the database in the initial data cleaning stage to eliminate the influence of process treatments. Two deep learning models were produced and the one incorporatingmanufacturing was found to perform better, highlighting the need to consider manufacturing conditions. Wang et al. [101] employed a similar approach, but instead utilised processing conditions, such as annealing time and temperature, as input features due to their influence on mechanical properties. Bakr et al. [45] trained a series of neural networks, using nominal MPEA composition, manufacturing route and heat treatment temperature as the direct input features in the hardness model.\n\nAs discussed, when MPEA ML studies experimentally validate the ML model predictions they fabricate a small sample of compositions [36]. However, the combination of large MPEA compositional space and capability of ML to rapidly make predictions across this unexplored space makes the connection of ML to High-Throughput Experiments (HTE) a logical advancement. HTE enable rapid synthesis and characterisations of MPEAs to validate ML predictions with enhanced efficiency [110]. Thus, the application of HTE approaches to ML has the potential to further accelerate and reduce cost of MPEA design [117]. Furthermore, the rapid generation of experimental data by implementation of HTEs can aid in constructing substantial MPEA databases for the development of future ML models [36]. Moorehead et al. [118] utilised additive\n\nFigure 9: Experimental analysis performed on a series of 5 different compositions within the FeNiCuCo alloy system, downselected using a random forest model. a) Hardness, b) X-ray diffraction. c) Back scattered electron images. d) Energy dispersive x-ray spectroscopy maps of elemental distributions. [40] ©2024 reused with permission from Elsevier\n\nmanufacturing in the form of directed energy deposition to construct arrays of different MPEA compositions onto a single build plate. 50 individual MPEA compositions are printed onto a single plate and x-ray diffraction, scanning electron microscopy and energy dispersive x-ray spectroscopy are all performed without removing the parts from the build plate. Moorehead et al. [118] also produced an arc-melted button, indicative of those typically produced by MPEA ML studies for validation and found that additive manufacturing enabled a time saving of an order of magnitude compared to conventional casting. Vecchio et al. [119] validated computational predictions of phase formation using CALPHAD, and hardness using an ML model with HTEs. MPEA samples were printed using directed energy deposition with a special hopper system allowing 16 different alloy compositions to be printed per build cycle. The MPEAs are built specifically to allow characterisation of microstructural and mechanical properties to also be performed in a high-throughput manner, Figure 10. In contrast, Liu et al. [36] applied HTE prior to ML to fabricate 138 MPEA compositions of the chosen alloy system through powder metallurgy. These compositions were then used to train an ML model to reveal the complete composition-hardness relationships across the compositional range of the chosen system. In addition, future expansion of HTE methodologies to include the implementation of autonomous or self-driving laboratories could further accelerate the discovery and optimisation of materials, [120, 121, 122] with MPEAs representing a suitable materials candidate.\n\n## Conclusions\n\nSupervised ML has the potential to accelerate materials design and discovery within the MPEA space. The current direction of travel in the application of ML to the MPEA field is towards the complete automation of the materials design process. ML can be used to make large numbers of predictions on MPEA compositions to identify those with potentially advantageous mechanical and structural properties. These identified compositions can then be fabricated in a high-throughput manner within automated laboratories to enable model validation and assess their suitability to meet the design criteria. Regardless of the level of automation, domain knowledge of materials science is still integral to ensure the application of ML within the field is logical and incorporates the established understandings of decades of research. Additionally, there are many gaps identified within the field that need to be addressed in the interest of standardising and enhancing the use of supervised ML for MPEA design.\n\nFirst and most critical, is the availability of MPEA data. Lack of available data is the most commonly cited reason for performance issues encountered in MPEA based ML studies. Hence, experimental MPEA data needs to be published and made open access to be utilised effectively in\n\nFigure 10: High-throughput experimental workflow from computation ML model predictions through alloy fabrication, characterisation, testing and analysis in an automated high-throughput manner, before closing the loop. [119] ©2024 reused with permission from Elsevier\n\nfuture work within the field. This must include the method of fabrication and testing to improve the reliability of the data and enable it to be incorporated into ML models. The CALPHAD and DFT based data used in some studies to train models should also be made available to save on computationally expensive and time-consuming calculations. Furthermore, a wider, more general issue within the MPEA field is the need for a standardised naming convention for MPEAs to be established, making pre-existing data on alloy compositions easier to discern.\n\nThe relationship between supervised ML based alloy design methodologies and traditional alloy design methodologies needs to be considered. As discussed and shown in Table 3 of this report, a majority of ML MPEA studies focus on the optimisation of a single alloy property, either within a preselected system or across the whole MPEA space. In contrast, conventional alloy design processes look to balance many properties to achieve high performance while minimising cost and maintaining manufacturability. Hence, there is a clear need to move to multi output models.\n\nFinally, the success and development of ML within the MPEA field is intrinsically linked to the experimental exploration of the compositional space. Many ML MPEA studies contain a workflow that portray data looping, where the experimental results of the study are fed back into the available data. However, the studies themselves end at the experimental assessments and publication. Thus, it is imperative that this data is not lost and is indeed recycled back into openly available databases to fuel further ML studies.\n\n## Acknowledgements\n\nThis work was supported by Oerlikon AM Europe GmbH (website: [https://www.oerlikon.com/am/en/](https://www.oerlikon.com/am/en/)), Engineering and Physical Sciences Research Council UK [EP/S022635/1] (website: [https://www.ukri.org/councils/epsrc](https://www.ukri.org/councils/epsrc)).\n\n## Declaration of conflicting interests\n\nThe authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.\n\n## Funding\n\nThe authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was supported by the Engineering and Physical Sciences Research Council, Oerlikon AM Europe GmbH, (grant number EP/S022635/1).\n\n## ORCID iDs\n\nJoshua Berry \n\n[https://orcid.org/0000-0001-7291-2306](https://orcid.org/0000-0001-7291-2306)\n\nKaterina A. Christofidou \n\n[https://orcid.org/0000-0002-8064-5874](https://orcid.org/0000-0002-8064-5874)\n\n## References\n\n* [1] Yeh JW, Chen SK, Lin SJ, et al. Nanostructured high-entropy alloys with multiple principal elements: novel alloy design concepts and outcomes. _Adv Eng Mater_ 2004; 6: 299-303.\n* [2] Cantor B, Chang ITH, Knight P, et al. Microstructural development in equatomic multicomponent alloys. _Materials Science & Engineering 4, Structural Materials: Properties, Microstructure and Processing_ 2004; 375-377: 213-218.\n* [3] Liu X, Zhang J and Pei Z. Machine learning for high-entropy alloys: progress, challenges and opportunities. _Progress in Materials Science_ 2023; 131: 101018..\n* [4] George EP, Raabe D and Ritchie RO. High-entropy alloys. _Nature Reviews Materials_ 2019; 4: 515-534.\n* [5] Miracle DB. Critical assessment I4: high entropy alloys and their development as structural materials. _Materials Science and Technology_ 2015; 31: 1142-1147.\n* [6] Miracle DB and Senkov ON. A critical review of high entropy alloys and related concepts. _Acta Materialia_ 2017; 122: 448-511.\n* [7] Miracle DB. High-Entropy alloys: a current evaluation of founding ideas and core effects and exploring \"nonlinear alloys\". _JOM_ 2017; 69: 2130-2136.\n* [8] Pickering E and Jones N. High-Entropy alloys: a critical assessment of their founding principles and future prospects. _International Materials Reviews_ 2016; 61: 183-202.\n* [9] Molnar C. _Interpretable Machine Learning: A Guide for Making Black Box Models Explainable_. 2nd Edition ed. 2022.\n* [10] Pilania G. Machine learning in materials science: from explainable predictions to autonomous design. _Computational Materials Science_ 2021; 193: 110360..\n* [11] Zhang Y, Zhou YJ, Lin JP, et al. Solid-Solution phase formation rules for multi-component alloys. _Adv Eng Mater_ 2008; 10: 534-538.\n* [12] Yang X and Zhang Y. Prediction of high-entropy stabilized solid-solution in multi-component alloys. _Materials Chemistry and Physics_ 2012; 132: 233-238.\n* [13] Guo S, Ng C, Lu J, et al. Effect of valence electron concentration on stability of fcc or bcc phase in high entropy alloys. _Journal of Applied Physics_ 2011; 109: 103505. doi: 10.1063/1.3587228\n* [14] Guo S and Liu CT. Phase stability in high entropy alloys: formation of solid-solution phase or amorphous phase. _Progress in Natural Science: Materials International_ 2011; 21: 433-446.\n* [15] Guo S, Hu Q, Ng C, et al. More than entropy in high-entropy alloys: forming solid solutions or amorphous phase. _Intermetallics_ 2013; 41: 96-103..\n* [16] Wang YP, Li BS and Fu HZ. Solid solution or intermetallics in a high-entropy alloy. _Advanced Engineering Materials_ 2009; 11: 641-644..\n* [17] Ren M-x, Li B-s and Fu H-z. Formation condition of solid solution type high-entropy alloy. _Transactions of Nonferous Metals Society of China_ 2013; 23: 991-995.\n* [18] Wang Z, Huang Y, Yang Y, et al. Atomic-size effect and solid solubility of multicomponent alloys. _Scripta Materialia_ 2015; 94: 28-31.\n* [19] Bracq G, Laurent-Brocq M, Perriere L, et al. The fcc solid solution stability in the Co-Cr-Fe-Mn-Ni multi-component system. _Acta Materialia_ 2017; 128: 327-336.\n* [20] Wu YD, Cai YH, Chen XH, et al. Phase composition and solid solution strengthening effect in TiZrNbMoV high-entropy alloys. _Materials & Design_ 2015; 83: 651-660.\n* [21] Liu WH, He JY, Huang HL, et al. Effects of Nb additions on the microstructure and mechanical property of CoCrFeNi high-entropy alloys. _Intermetallics_ 2015; 60: -8.\n\n* [22] Christofidou KA, McAuliffe TP, Mignanelli PM, et al. On the prediction and the formation of the sigma phase in CrMnCoFeNix high entropy alloys. _Journal of Alloys and Compounds_ 2019; 770: 285-293.\n* [23] Christofidou KA, Pickering EJ, Orsatti P, et al. On the influence of Mn on the phase stability of the CrMnxFeCoNi high entropy alloys. _Intermetallics_ 2018; 92: 84-92.\n* [24] Rogal L, Szklarz Z, Bobrowski P, et al. Microstructure and mechanical properties of Al-Co-Cr-Fe-Ni base high entropy alloys obtained using powder metallurgy. _Metals and Materials International_ 2019; 25: 930-945.\n* [25] Morgan D and Jacobs R. Opportunities and challenges for machine learning in materials science. _Annual Review of Materials Research_ 2020; 50: 71-103.\n* [26] Hart GLW, Mueller T, Toher C, et al. Machine learning for alloys. _Nature Reviews Materials_ 2021; 6: 730-755.\n* [27] Huang W, Martin P and Zhuang HL. Machine-learning phase prediction of high-entropy alloys. _Acta Materialia_ 2019; 169: 225-236.\n* [28] Kaufmann K and Vecchio KS. Searching for high entropy alloys: a machine learning approach. _Acta Materialia_ 2020; 198: 178-222.\n* [29] Yan Y, Lu D and Wang K. Accelerated discovery of single-phase refractory high entropy alloys assisted by machine learning. _Computational Materials Science_ 2021; 199: 110723.\n* [30] Wang AY-T, Murdock RJ, Kauwe SK, et al. Machine learning for materials scientists: an introductory guide toward best practices. _Chemistry of Materials_ 2020; 32: 4954-4965.\n* [31] Maglogiannis IG. _Emerging artificial intelligence applications in computer engineering (electronic resource): real word AI systems with applications in eHealth, HCI, information retrieval and pervasive technologies_. Amsterdam Washington, DC: IOS Press, 2007.\n* [32] Zhu W, Huo W, Wang S, et al. Machine learning-based hardness prediction of high-entropy alloys for Laser additive manufacturing. _Jom_ 2023; 75: 5537-5548.\n* [33] Ren W, Zhang Y-F, Wang W-L, et al. Prediction and design of high hardness high entropy alloy through machine learning. _Materials & Design_ 2023; 235: 112454. doi: 10.1016/j.matdes.2023.112454\n* [34] Han Q, Lu Z, Zhao S, et al. Data-driven based phase constitution prediction in high entropy alloys. _Computational Materials Science_ 2022; 215: 111774.\n* [35] Vazquez G, Chakravarty S, Gurrola R, et al. A deep neural network regressor for phase constitution estimation in the high entropy alloy system al-co-cr-fe-mn-nb-Ni. _npj Computational Materials_ 2023; 9: 68.\n* [36] Liu Y, Wang J, Xiao B, et al. Accelerated development of hard high-entropy alloys with data-driven high-throughput experiments. _Journal of Materials Informatics_ 2022; 2: 3.\n* [37] Zhang J, Cai C, Kim G, et al. Composition design of high-entropy alloys with deep sets learning. _npj Computational Materials_ 2022; 8: 89. doi: 10.1038/s41524-022-00779-7\n* [38] Shen L, Chen L, Huang J, et al. Predicting phases and hardness of high entropy alloys based on machine learning. _Intermetallics_ 2023; 162: 108030.\n* [39] Akhil B, Bajpai A, Gurao NP, et al. Designing hexagonal close packed high entropy alloys using machine learning. _Modelling Simul Mater Sci Eng_ 2021; 29: 85005.\n* [40] Huang X, Jin C, Zhang C, et al. Machine learning assisted modelling and design of solid solution hardened high entropy alloys. _Materials & Design_ 2021; 211: 110177.\n* [41] Chen C, Ma L, Zhang Y, et al. Accelerating the design of high-entropy alloys with high hardness by machine learning based on particle swarm optimization. _Intermetallics_ 2023; 154: 107819.\n* [42] Islam N, Huang W and Zhuang HL. Machine learning for phase selection in multi-principal element alloys. _Computational Materials Science_ 2018; 150: 230-235.\n* [43] Kaufmann K, Maryanovsky D, Mellor WM, et al. Discovery of high-entropy ceramics via machine learning. _npj Computational Materials_ 2020; 6: 42.\n* [44] Bhandari U, Rafi MR, Zhang C, et al. Yield strength prediction of high-entropy alloys using machine learning. _Materials Today Communications_ 2021; 26: 101871. doi: 10.1016/j.mtcomm.2020.101871\n* [45] Bakr M, Syarif J and Hashem IAT. Prediction of phase and hardness of HEAs based on constituent elements using machine learning models. _Materials Today Communications_ 2022; 31: 103407. doi: 10.1016/j.mtcomm.2022.103407\n* [46] Jain R, Lee U, Samal S, et al. Machine-learning-guided phase identification and hardness prediction of Al-Co-Cr-Fe-Mn-Nb-Ni-V containing high entropy alloys. _Journal of Alloys and Compounds_ 2023; 956: 170193.\n* [47] Zhang L, Chen H, Tao X, et al. Machine learning reveals the importance of the formation enthalpy and atom-size difference in forming phases of high entropy alloys. _Materials & Design_ 2020; 193: 108835. doi: 10.1016/j.matdes.2020.108835\n* [48] Debnath A, Krajewski AM, Sun H, et al. Generative deep learning as a tool for inverse design of high entropy refractory alloys. _Journal of Materials Informatics_ 2021; 1: 3.\n* [49] Jain A, Ong SP, Hautier G, et al. Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. _APL Materials_ 2013; 1: 011002. doi: 10.1063/1.4812323\n* [50] Ye YF, Wang Q, Lu J, et al. High-entropy alloy: challenges and prospects. _Materials Today_ 2016; 19: 349-362.\n* [51] Gorse S, Nguyen MH, Senkov ON, et al. Database on the mechanical properties of high entropy alloys and complex concentrated alloys. _Data Brief_ 2018; 21: 2664-2678.\n* [52] Gorse S, Nguyen MH, Senkov ON, et al. Corrigendum to database on the mechanical properties of high entropy alloys and complex concentrated alloys, data in brief 21 (2018) 2664-2678. _Data in Brief_ 2020; 32: 106216-106216.\n* [53] Couzinic JP, Senkov ON, Miracle DB, et al. Comprehensive data compilation on the mechanical properties of refractory high-entropy alloys. _Data in Brief_ 2018; 21: 1622-1641.\n* [54] Balasubramanian A. Phases and Young's modulus dataset for High Entropy alloys. In: National Energy Technology Laboratory (NETL) P, PA, Morgenatown, WV, and Albany, OR (United States). Energy Data eXchange; National Energy Technology Laboratory (NETL), Pittsburgh, PA, Morgenatown, WV (United States), (ed.) USDOE Office of Scientific and Technical Information2020.\n* [55] Borg CKH, Frey C, Moh J, et al. Expanded dataset of mechanical properties and observed phases of multi-principal element alloys. _Scientific Data_ 2020; 7: 30.\n* [56] Machaka R, Motsi GT, Raganya LM, et al. Machine learning-based prediction of phases in high-entropy alloys: a data article. _Data in Brief_ 2021; 38: 107346-107346.\n* [57] Detor A, Oppenheimer S, Casey R, et al. Refractory high entropy alloy dataset with room temperature ductility screening. _Data in Brief_ 2022; 45: 108582.\n\n* [58] Chen S, Fan X, Steingrimsson B, et al. Fatigue dataset of high-entropy alloys. _Sci Data_ 2022; 9: 381. 20220706.\n* [59] Pei Z, Yin J, Hawk JA, et al. Machine-learning informed prediction of high-entropy solid solution formation: Beyond the Hume-Rotherny rules. _npj computational materials_ 2020; 6: 50. doi: 10.1038/s41524-020-0308-7\n* [60] Beniwal D, Singh P, Gupta S, et al. Distilling physical origins of hardness in multi-principal element alloys directly from ensemble neural network models. _npj Computational Materials_ 2022; 8: 153. doi: 10.1038/s41524-022-00842-3\n* [61] Bundlea AS and Rahul MR. Machine learning-enabled framework for the prediction of mechanical properties in new high entropy alloys. _Journal of Alloys and Compounds_ 2022; 908: 164578. doi: 10.1016/j.jallcom.2022.164578\n* [62] Ottomano F, De Felice G, Gusev VV, et al. Not as simple as we thought a rigorous examination of data aggregation in materials informatics. _Digital Discovery_ 2024; 3: 337-346.\n* [63] Lee SY, Byeon S, Kim HS, et al. Deep learning-based phase prediction of high-entropy alloys: Optimization, generation, and explanation. _Materials & Design_ 2021; 197: 109260. doi: 10.1016/j.matdes.2020.109260\n* [64] Gilligan LPJ, Cobelli M, Taufour V, et al. A rule-free workflow for the automated generation of databases from scientific literature. _npj Computational Materials_ 2023; 9: 222. doi: 10.1038/s41524-023-01171-9\n* [65] Dagdelen J, Dunn A, Lee S, et al. Structured information extraction from scientific text with large language models. _Nature Communications_ 2024; 15: 1418. doi: 10.1038/s41467-024-45563-x\n* [66] Risal S, Zhu W, Guillen P, et al. Improving phase prediction accuracy for high entropy alloys with Machine learning. _Computational Materials Science_ 2021; 192: 110389. doi: 10.1016/j.commatsci.2021.110389\n* [67] Elredy D and Atiya AF. A comprehensive analysis of synthetic minority oversampling technique (SMOTE) for handling class imbalance. _Information Sciences_ 2019; 505: 32-64.\n* [68] Bansal A, Kumar P, Yadav S, et al. Accelerated design of high entropy alloys by integrating high throughput calculation and machine learning. _Journal of Alloys and Compounds_ 2023; 960: 170543. doi: 10.1016/j.jallcom.2023.170543\n* [69] Singh S, Katiyar NK, Goel S, et al. Phase prediction and experimental realisation of a new high entropy alloy using machine learning. _Sci Rep_ 2023; 13: 4811. 20230323.\n* [70] Hareharen K, Panneerselvam T and Raj Mohan R. Improving the performance of machine learning model predicting phase and crystal structure of high entropy alloys by the synthetic minority oversampling technique. _Journal of Alloys and Compounds_ 2024; 991: 174494. doi: 10.1016/j.jallcom.2024.174494\n* [71] Hastie T, Friedman JH and Tibshirani R. _The elements of statistical learning : data mining, inference, and prediction_. 2nd ed. New York: Springer, 2009.\n* [72] Krishna YV, Jaiswal UK and R RM. Machine learning approach to predict new multiphase high entropy alloys. _Scripta Materialia_ 2021; 197: 113804.\n* [73] Kube SA, Sohn S, Uhl D, et al. Phase selection motifs in high entropy alloys revealed through combinatorial methods: large atomic size difference favors BCC over FCC. _Acta Materialia_ 2019; 166: 677-686.\n* [74] Tian F, Varga LK, Chen N, et al. Empirical design of single phase high-entropy alloys with high hardness. _Intermetallics_ 2015; 58: -6.\n* [75] Fang S, Xiao X, Xia L, et al. Relationship between the widths of supercooled liquid regions and bond parameters of Mg-based bulk metallic glasses. _Journal of non-Crystalline Solids_ 2003; 321: 120-125.\n* [76] Mizutani U and Sato H. The physics of the Hume-Rothery electron concentration rule. _Crystals (Based)_ 2017; 7: 9.\n* [77] Calvo-Dahlborg M, Mehraban S, Lavery NP, et al. Prediction of phase, hardness and density of high entropy alloys based on their electronic structure and average radius. _Journal of Alloys and Compounds_ 2021; 865: 158799.\n* [78] Takeuchi A and Inoue A. Classification of bulk metallic glasses by atomic size difference, heat of mixing and period of constituent elements and its application to characterization of the main alloying element. _Mater Trans_ 2005; 46: 2817-2829.\n* [79] Rickman JM, Balasubramanian G, Marvel Cl, et al. Machine learning strategies for high-entropy alloys. _Journal of Applied Physics_ 2020; 128: 221101. doi: 10.1063/5.0030367\n* [80] Callister WD and Rethwisch DG. _Materials science and engineering. Ninth edition, SI version. ed._ Hoboken, New Jersey, Hoboken, NJ: Wiley, 2015.\n* [81] Prieto E, Vaz-Romero A, Gonzalez-Julian J, et al. Novel high entropy alloys as binder in cernets: from design to sintering. _International Journal of Refuctory Metals & Hard Materials_ 2021; 99: 105592.\n* [82] Gwalani B, Choudhuri D, Liu K, et al. Interplay between single phase solid solution strengthening and multi-phase strengthening in the same high entropy alloy. _Materials Science & Engineering A. Structural Materials : Properties, Microstructure and Processing_ 2020; 771: 138620.\n* [83] George EP, Curtin WA and Tasan CC. High entropy alloys: a focused review of mechanical properties and deformation mechanisms. _Acta Materialia_ 2020; 188: 435-474.\n* [84] Yang C, Ren C, Jia Y, et al. A machine learning-based alloy design system to facilitate the rational design of high entropy alloys with enhanced hardness. _Acta Materialia_ 2022; 222: 117431.\n* [85] Zeng Y, Man M, Bai K, et al. Revealing high-fidelity phase selection rules for high entropy alloys: a combined CALPHAD and machine learning study. _Materials & Design_ 2021; 202: 109532.\n* [86] Wen C, Zhang Y, Wang C, et al. Machine learning assisted design of high entropy alloys with desired property. _Acta Materialia_ 2019; 170: 109-117.\n* [87] Zhang Y, Wen C, Wang C, et al. Phase prediction in high entropy alloys with a rational selection of materials descriptors and machine learning models. _Acta Materialia_ 2020; 185: 528-539.\n* [88] Li S, Li S, Liu D, et al. Hardness prediction of high entropy alloys with machine learning and material descriptors selection by improved genetic algorithm. _Computational Materials Science_ 2022; 205: 111185.\n* [89] Machaka R. Machine learning-based prediction of phases in high-entropy alloys. _Computational Materials Science_ 2021; 188: 110244.\n* [90] James G, Witten D, Hastie T, et al. _An introduction to statistical learning : with applications in R_. New York: New York: Springer, 2013.\n* [91] Campbell MJ. _Statistics at square one._ Twelfth edition. Campbell Michael J. (ed.) Hoboken, NJ: Wiley Blackwell, 2021, pp.165-169.\n\n* [92] Akoglu H. User's guide to correlation coefficients. _Turk J Emerg Med_ 2018; 18: 91-93. 20180807.\n* [93] Papageorgiou SN. On correlation coefficients and their interpretation. _Journal of Orthodontics_ 2022; 49: 359-361.\n* [94] Choudhury A, Konnur T, Chattopadhyay PP, et al. Structure prediction of multi-principal element alloys using ensemble learning. _Engineering Computations_ 2020; 37: 1003-1022.\n* [95] Dalianis H. Evaluation metrics and evaluation. In: Dalianis H (eds) _Clinical text mining: secondary use of electronic patient records_. Cham: Springer International Publishing, 2018, pp.45-53.\n* [96] Chang Y-J, Jui C-Y, Lee W-J, et al. Prediction of the composition and hardness of high-entropy alloys by machine learning. _JOM_ 2019; 71: 3433-3442.\n* [97] Roy A, Babuska T, Krick B, et al. Machine learned feature identification for predicting phase and Young's modulus of low-, medium- and high-entropy alloys. _Scripta Materialia_ 2020; 185: 152-158.\n* [98] Revi V, Kasodariya S, Talapatra A, et al. Machine learning elastic constants of multi-component alloys. _Computational Materials Science_ 2021; 198: 110671. doi: 10.1016/j.commatsci.2021.110671\n* [99] Liu F, Xiao X, Huang L, et al. Design of NiCoCrAl eutectic high entropy alloys by combining machine learning with CALPHAD method. _Materials Today Communications_ 2022; 30: 103172.\n* [100] Sai NJ, Rathore P and Chauhan A. Machine learning-based predictions of fatigue life for multi-principal element alloys. _Scripta Materialia_ 2023: 226. DOI: 10.1016/j.scriptamat.2022.115214\n* [101] Wang J, Kwon H, Kim HS, et al. A neural network model for high entropy alloy design. _npj Computational Materials_ 2023; 9: 60.\n* [102] Berry J, Snell R, Anderson M, et al. Design and selection of high entropy alloys for hardmetal matrix applications using a coupled machine learning and calculation of phase diagrams methodology. _Advanced Engineering Materials_ 2024; 26: 2302064. doi: 10.1002/adem.202302064\n* [103] Huang X, Zheng L, Xu H, et al. Predicting and understanding the ductility of BCC high entropy alloys via knowledge-integrated machine learning. _Materials & Design_ 2024; 239: 112797. doi: 10.1016/j.matdes.2024.112797\n* [104] Lee C-Y, Jui C-Y, Yeh A-C, et al. Inverse design of high entropy alloys using a deep interpretable scheme for materials attribution analysis. _Journal of Alloys and Compounds_ 2024; 976: 173144. doi: 10.1016/j.jallcom.2023.173144\n* [105] Zeng Y, Man M, Ng CK, et al. Machine learning-based inverse design for single-phase high entropy alloys. _APL Materials_ 2022; 10: 112929. doi: 10.1063/5.0109491\n* [106] Guo Q, Pan Y, Hou H, et al. Predicting the hardness of high-entropy alloys based on compositions. _International Journal of Refractory Metals and Hard Materials_ 2023; 112. DOI: 10.1016/j.ijmmhm.2023.106116\n* [107] Debnath A, Raman L, Li W, et al. Comparing forward and inverse design paradigms: a case study on refractory high-entropy alloys. _Journal of Materials Research_ 2023; 38: 4107-4117.\n* [108] Lee J, Park D, Lee M, et al. Machine learning-based inverse design methods considering data characteristics and design space size in materials design and manufacturing: a review. _Mater Horiz_ 2023; 10: 5436-5456. 20231127.\n* [109] Kaushik N, Meena A and Mali HS. High entropy alloy synthesis, characterisation, manufacturing & potential applications: a review. _Materials and Manufacturing Processes_ 2021; 37: 1085-1109.\n* [110] Zhang Y and Xing Q. High entropy alloys: manufacturing routes. In: Caballero FG (eds) _Encyclopedia of materials: metals and alloys_. Oxford: Elsevier, 2022, pp.327-338.\n* [111] Alshatiari YA, Sivasankaran S, Al-Mufadi FA, et al. Manufacturing methods, microstructural and mechanical properties evolutions of high-entropy alloys: a review. _Metals and Materials International_ 2019; 26: 1099-1133.\n* [112] Zhang W, Chabok A, Kooi BJ, et al. Additive manufactured high entropy alloys: A review of the microstructure and properties. _Materials & Design_ 2022; 220: 110875. doi: 10.1016/j.matdes.2022.110875\n* [113] Otto F, Dlouhy A, Pradeep KG, et al. Decomposition of the single-phase high-entropy alloy CrMnFeCoNi after prolonged anneals at intermediate temperatures. _Acta Materialia_ 2016; 112: 40-52.\n* [114] Jablonski PD, Licavi JJ, Gao MC, et al. Manufacturing of high entropy alloys. _Jom_ 2015; 67: 2278-2287.\n* [115] Nagase T, Mizuuchi K and Nakano T. Solidification microstructures of the ingots obtained by arc melting and cold crucible levitation melting in TiNbTaZr Medium-entropy alloy and TiNbTaZrX (X = V, Mo, W) high-entropy alloys. _Entropy (Basel)_ 2019; 21: 20190510.\n* [116] Wang Z, Li L, Chen Z, et al. A new route to achieve high strength and high ductility compositions in Cr-Co-Ni-based medium-entropy alloys: a predictive model connecting theoretical calculations and experimental measurements. _Journal of Alloys and Compounds_ 2023; 959: 170555.\n* [117] Huang EW, Lee W-J, Singh SS, et al. Machine-learning and high-throughput studies for high-entropy materials. _Materials Science and Engineering: R: Reports_ 2022; 147: 100645. doi: 10.1016/j.mser.2021.100645\n* [118] Moorehead M, Bertsch K, Niezgoda M, et al. High-throughput synthesis of Mo-Nb-Ta-W high-entropy alloys via additive manufacturing. _Materials & Design_ 2020; 187: 108358. doi: 10.1016/j.matdes.2019.108358\n* [119] Vecchio KS, Dippo OF, Kaufmann KR, et al. High-throughput rapid experimental alloy development (HT-READ). _Acta Materialia_ 2021; 221. doi: 10.1016/j.actamat.2021.117352\n* [120] Baird SG and Sparks TD. What is a minimal working example for a self-driving laboratory? _Matter_ 2022; 5: 4170-4178.\n* [121] MacLeod BP, Parlane FGL, Rupnow CC, et al. A self-driving laboratory advances the Pareto front for material properties. _Nat Commun_ 2022; 13: 995. 20220222.\n* [122] Szymanski NJ, Rendy B, Fei Y, et al. An autonomous laboratory for the accelerated synthesis of novel materials. _Nature_ 2023; 624: 86-91. 20231129."
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bMWJUfuoSij_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}